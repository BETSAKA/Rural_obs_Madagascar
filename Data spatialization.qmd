---
title: "Appendix: data spatialization"
author: "Florent Bédécarrats"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

## Cleanig and harmonizing the municipality

### Preparing the Data for Municipality Matching

To accurately merge datasets with varying administrative names, we often have to utilize a combination of automated processes and manual review. Our approach consists of the following major steps:

1.  **Reading and Preprocessing Survey Data:** Initial handling of raw survey data to extract and standardize municipality names.
2.  **Processing GADM Data:** GADM offers a comprehensive spatial dataset, which we leverage for authoritative municipality names.
3.  **Fuzzy Matching:** Given the potential discrepancies between names in different datasets, a fuzzy string matching technique is applied to assess the degree in which the municipality names found in the ROR survey differ from their closest match in GADM dataset.
4.  **Computing Relative Distance and Preparing for Manual Review:** The relative difference between matched strings is used to guide our manual review process.

```{r}
# Load required libraries
library(tidyverse)
library(haven)
library(labelled)
library(geodata)
library(sf)
library(stringdist)
```

## Extract a list of location values

```{r}
# Define path to the survey location data
deb_files <- list.files(path = "enter", pattern = "res_deb", 
                        recursive = TRUE, 
                        full.names = TRUE) %>%
  str_subset("stunicode", negate = TRUE) 

# Define variables related to locations
location_variables <- list(
  year = "data collection year",
  j0 = "observatory code",
  obs = "observatory",
  j41 = "municipality code",
  j42 = "municipality name",
  j4_code = "village code",
  j4 = "village name",
  code_site = "site code")

# Extract location-related data from the survey files and preprocess
location_values <- deb_files %>%
  map(~ read_dta(.x) %>%
        mutate(across(where(is.labelled), ~ as.character(as_factor(.))))) %>%
  map(~ select(.x, any_of(names(location_variables)))) %>%
  map(~ .x %>% mutate_all(as.character)) %>%
  map(unique) %>%
  bind_rows() %>%
  rename(obs_code = j0, muni_code = j41, muni = j42, hamlet_code = j4_code,
         hamlet = j4, site_code = code_site)
```

# Compare to official lists of localities

```{r}

# Harmonize names
location_values2 <- location_values %>%
  mutate(across(where(is.character), ~ str_to_upper(.))) %>%
  mutate(across(where(is.character), ~str_replace_all(., "/", " "))) %>%
  mutate(across(where(is.character), ~str_remove_all(., " CENTRE")))

# Check municipality names
munis_ror_names <- location_values2 %>%
  select(muni) %>%
  unique() %>%
  pluck("muni")


munis_gadm <- gadm("MDG", level  = 4, path = "data") %>%
  st_as_sf()

munis_gadm_names <- munis_gadm %>%
  st_drop_geometry() %>%
  select(muni = NAME_4) %>%
  mutate(muni = str_to_upper(muni)) %>%
  pluck("muni")



# Compute Levenshtein distance to find closest match in GADM
distance_matrix <- stringdistmatrix(munis_ror_names, munis_gadm_names, 
                                    method = "lv")
# write_rds(distance_matrix, "data/distance_matrix_munis.rds")


closest_matches <- apply(distance_matrix, 1, which.min) %>%
  as.integer()
proximity_grades <- apply(distance_matrix, 1, min) %>%
  as.numeric()
muni_names_matches <- tibble(munis_ror = munis_ror_names,
                             closest_gadm_name = munis_gadm_names[closest_matches],
                             proximity_grade = proximity_grades)

 #writexl::write_xlsx(muni_names_matches, "muni_names_matches.xlsx")
# Match to existing municipalities
```

Attempt on 2007

```{r}
# Prepare data for 2007
loc_2007 <- location_values %>%
  filter(year == 2007) %>%
  mutate(muni_ror = str_to_upper(muni)) %>%
  mutate(muni_ror = str_replace_all(muni_ror, "/", " ")) %>%
  mutate(muni_ror = str_remove_all(muni_ror, " CENTRE"))

# Check municipality names
munis_ror2007_names <- loc_2007 %>%
  select(muni_ror) %>%
  unique() %>%
  pluck("muni_ror")

munis_gadm <- gadm("MDG", level  = 4, path = "data") %>%
  st_as_sf() %>%
  mutate(muni = str_to_upper(NAME_4))

munis_gadm_names <- munis_gadm %>%
  pluck("muni")

distance_matrix <- stringdistmatrix(munis_ror2007_names, munis_gadm_names, 
                                    method = "lv")
closest_matches <- apply(distance_matrix, 1, which.min) %>%
  as.integer()
proximity_grades <- apply(distance_matrix, 1, min) %>%
  as.numeric()

muni_names_matches_2007 <- tibble(
  muni_ror = munis_ror2007_names,
  closest_gadm_name = munis_gadm_names[closest_matches],
  proximity_grade = proximity_grades) %>%
  mutate(verif = "")
# Some matches proposed by this technique are obviously wrong (11/109)
# We corrected them manually
# writexl::write_xlsx(muni_names_matches_2007, "muni_names_matches_2007.xlsx")
manual_matches <- tibble::tribble(
                     ~muni_ror,     ~manual_match,              ~source_match,
          "FERAMANGA-AVARATRA",     "AMBANDRIKA",      "OCHA located places",
          "FERAMANGA AVARATRA",     "AMBANDRIKA",      "OCHA located places",
                   "AMBODIADY",   "AMBODIFARIHY",                   "Visual",
                 "ANDROKAVATO",         "AMBANO", "agritrop.cirad.fr/558679",
"VOHITROMBY VOHITROMBY CENTRE",     "VOHITROMBY",                   "Visual",
   "AMBATOHARANANA MAHANTSARA", "AMBATOHARANANA",                   "Visual",
 "AMBATOHARANANA AMBODIBARIKA", "AMBATOHARANANA",                   "Visual",
                    "BEZEZIKA",     "ANKILIVALO",      "OCHA located places",
                    "BEKININY",         "BEFASY",      "OCHA located places",
                "FORT DAUPHIN",       "TOLANARO",                "Wikipedia",
              "TSIVORY CENTRE",        "TSIVORY",                   "Visual")

# We use the automatic match or manual one when we created one
muni_names_matches_2007 <- muni_names_matches_2007 %>%
  left_join(select(manual_matches, muni_ror, manual_match),
            by = join_by("muni_ror")) %>%
  mutate(gadm_match = ifelse(is.na(manual_match), 
                             closest_gadm_name, manual_match)) %>%
  select(muni_ror, gadm_match) %>%
  left_join()

# Get the original names back into loc_2007
loc_2007_2 <- muni_names_matches_2007 %>%
  left_join(munis_gadm %>%
              st_drop_geometry() %>%
              select(gadm_match = muni, name_gadm = NAME_4),
            by = join_by("gadm_match")) 


munis_gadm %>%
  st_drop_geometry() %>%
  group_by(NAME_1, NAME_4) %>%
  summarize(count = n()) %>%
  filter(count > 1)



# Andrakavato est un hameau de la commune d'Ambano
# Source: https://agritrop.cirad.fr/558679/1/document_558679.pdf
```

```{r}
# Define a function to load and count surveys per observatory for a given year
load_and_count <- function(year) {
  # Define file path
  file_path <- paste0("enter/", year, "/res_deb.dta")
  
  # Load data
  data <- read_dta(file_path)
  
  # Count surveys per observatory
  count_data <- data %>%
    group_by(j0) %>%
    summarise(survey_count = n()) %>%
    ungroup() %>%
    mutate(year = year)  # Add year column
  
  return(count_data)
}

# Generate a list of years
years <- 1995:2015

# Use purrr::map_df to loop through each year and bind results
result <- map_df(years, load_and_count)

result_wide <- result %>%
  pivot_wider(names_from = year, values_from = survey_count)

# Print the result
print(result)
```

## Create a data dictionnary

```{r}
# Function to extract variable info for a given year and file
extract_variable_info <- function(year, file) {
  # Define file path
  file_path <- paste0("enter/", year, "/", file)
  
  # Check if the file exists
  if (!file.exists(file_path)) return(NULL)
  
  # Load data (but only headers, not full data to speed up processing)
  data <- read_dta(file_path, n_max = 0)
  
  # Extract variable names and labels
  var_names <- names(data)
  var_labels <- var_label(data)
  
  # Create a tibble to store extracted data
  tibble(
    file_name = file,
    variable_name = var_names,
    variable_label = var_labels,
    year = year
  )
}

# Use purrr::map_df to loop through each year, list its files, and extract info
all_vars <- map_df(years, function(y) {
  # List files for the specific year
  files_for_year <- list.files(paste0("enter/", y), pattern = "\\.dta$", full.names = FALSE)
  # Apply extract_variable_info function for each file of the year
  map_df(files_for_year, extract_variable_info, year = y)
})

# Convert any NULL values in variable_label to "NA"
all_vars$variable_label <- as.character(all_vars$variable_label)
all_vars$variable_label[is.na(all_vars$variable_label)] <- "NA"

# Consolidate the information
variable_dictionary <- all_vars %>%
  group_by(file_name, variable_name) %>%
  arrange(year) %>%  # Arranging by year can ensure that we capture the most frequent or earliest label
  summarise(
    variable_label = first(variable_label[variable_label != "NA"] %||% "NA"),
    years_present = list(unique(year))
  ) %>%
  ungroup()


# Print the variable dictionary
print(variable_dictionary)
# Convert the list column to a character column
variable_dictionary$years_present <- sapply(variable_dictionary$years_present, paste, collapse = ",")

# Now split and save the chunks to CSV as before:
num_rows <- nrow(variable_dictionary)
chunk_size <- 100

split_data <- split(variable_dictionary, ceiling(seq_len(num_rows)/chunk_size))

for(i in seq_along(split_data)) {
  utils::write.table(split_data[[i]], file = paste0("chunk_", i, ".csv"), row.names = FALSE, sep = ",", quote = TRUE)
}

```
