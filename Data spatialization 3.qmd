---
title: "Appendix: data spatialization"
author: "Florent Bédécarrats"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

## Cleanig and harmonizing the municipality

### Preparing the Data for Municipality Matching

To accurately merge datasets with varying administrative names, we often have to utilize a combination of automated processes and manual review. Our approach consists of the following major steps:

1. **Reading and Preprocessing Survey Data:** Initial handling of raw survey data to extract and standardize municipality names.
2. **Processing GADM Data:** GADM offers a comprehensive spatial dataset, which we leverage for authoritative municipality names.
3. **Fuzzy Matching:** Given the potential discrepancies between names in different datasets, a fuzzy string matching technique is applied to assess the degree in which the municipality names found in the ROR survey differ from their closest match in GADM dataset.
4. **Computing Relative Distance and Preparing for Manual Review:** The relative difference between matched strings is used to guide our manual review process.

```{r}
# Load required libraries
library(tidyverse)
library(haven)
library(labelled)
library(geodata)
library(sf)
library(stringdist)
```

## Extract a list of location values

```{r}
# Define path to the survey location data
deb_files <- list.files(path = "enter", pattern = "res_deb", 
                        recursive = TRUE, 
                        full.names = TRUE) %>%
  str_subset("stunicode", negate = TRUE) 

# Define variables related to municipalty locations
location_variables <- list(
  j0 = "observatory code",
  obs = "observatory",
  j41 = "municipality code",
  j42 = "municipality name",
  j4_code = "village code",
  j4 = "village name",
  code_site = "site code")

# Extract location-related data from the survey files and preprocess
location_values <- deb_files %>%
  map(~ read_dta(.x) %>%
        mutate(across(where(is.labelled), ~ as.character(as_factor(.))))) %>%
  map(~ select(.x, any_of(names(location_variables)))) %>%
  map(~ .x %>% mutate_all(as.character)) %>%
  map(unique) %>%
  bind_rows() %>%
  rename(obs_code = j0, muni_code = j41, muni_ror = j42, hamlet_code = j4_code,
         hamlet = j4, site_code = code_site) %>%
  mutate(muni = str_to_upper(muni_ror),
         muni = str_replace_all(muni, "/", " "),
         muni = str_remove_all(muni, " CENTRE")) %>%
  unique()
```


As a resukts we have a list of all variation of location variables found in the data. We have a total of `r nrow(location_values)` different commune names.

### Processing GADM Data

The GADM (Global Administrative Areas) database provides up-to-date administrative boundaries and names for all countries globally. Here, we extract the municipal names for Madagascar. These serve as our reference set in the subsequent matching process.

```{r}
munis_gadm <- gadm("MDG", level  = 4, path = "data") %>%
  st_as_sf() %>%
  mutate(muni = str_to_upper(NAME_4))
```


### Fuzzy Matching Process

Given that different datasets might have slight variations in naming conventions or typographical differences, a simple direct match can often overlook correct pairings. By employing a fuzzy string matching method, we can capture these near-matches more effectively. Here, we compute a distance matrix to identify the closest possible matches between our two sets of municipality names.

```{r fuzzy_matching}
# Compute the distance matrix between municipality names in the survey data and GADM data
distance_matrix <- stringdistmatrix(location_values$muni, munis_gadm$muni, 
                                    method = "lv")

# Identify closest matches based on the distance matrix
closest_matches <- apply(distance_matrix, 1, which.min) %>%
  as.numeric()
proximity_grades_abs <- apply(distance_matrix, 1, min)
proximity_grades_rel <- proximity_grades_abs / str_length(location_values$muni) * 100

# Bind the municipality names, their closest matches, and the proximity grades into a tibble
muni_matches <- tibble(
  obs_code = location_values$obs_code,
  muni_ror = location_values$muni_ror,
  muni = location_values$muni,
  hamlet_code = location_values$hamlet_code,
  hamlet = location_values$hamlet,
  closest_gadm_name = munis_gadm$muni[closest_matches],
  closest_gadm_gid = munis_gadm$GID_4[closest_matches],
  closest_gadm_district = munis_gadm$NAME_3[closest_matches],
  closest_gadm_region = munis_gadm$NAME_2[closest_matches],
  proximity_grade_abs = proximity_grades_abs,
  proximity_grade_rel = proximity_grades_rel) %>%
  arrange(desc(proximity_grade_rel)) # Order by relative distance

# Export the dataset for manual review
writexl::write_xlsx(muni_matches, "review_muni_matches.xlsx")

```

We computed the Levenshtein distance between each municipality name from the survey data and the GADM data. Based on this distance, we identify the closest match from the GADM dataset for each survey municipality name. The proximity grade is calculated to guide potential manual review.


We check observatory by observatory to identify similar names of naming patterns.
To disambiguate some location names, we also use the Populated places database from the OCHA (https://data.humdata.org/fr/dataset/madagascar-settlements)


### Re-incorporate the corrected matches

```{r}
# Reading the corrected file
corrected_matches <- readxl::read_xlsx("review_muni_matches_correct.xlsx")


# 3. Integrating Corrections
muni_matches_updated <- corrected_matches %>%
  mutate(
    closest_gadm_name = ifelse(!is.na(NAME_4_correct), NAME_4_correct, 
                               closest_gadm_name),
    closest_gadm_gid = ifelse(!is.na(GID_4_correct), GID_4_correct, 
                              closest_gadm_gid)) %>%
  select(-NAME_4_correct, -GID_4_correct, -Source_correct, -proximity_grade_abs,
         -proximity_grade_rel) # dropping unnecessary columns

# Define variables related to municipalty locations
loc_vars_w_years <- list(
  year = "data collection year",
  j0 = "observatory code",
  obs = "observatory",
  j41 = "municipality code",
  j42 = "municipality name",
  j4_code = "village code",
  j4 = "village name",
  code_site = "site code")

# Recreate a similar dataset, with one observation per year
location_values_all <- deb_files %>%
  map(~ read_dta(.x) %>%
        mutate(across(where(is.labelled), ~ as.character(as_factor(.))))) %>%
  map(~ select(.x, any_of(names(loc_vars_w_years)))) %>%
  map(~ .x %>% mutate_all(as.character)) %>%
  bind_rows() %>%
  rename(obs_code = j0, muni_ror = j42, hamlet_code = j4_code, hamlet = j4) %>%
  group_by(year, obs_code, muni_ror, hamlet_code, hamlet) %>%
  summarise(count = n()) %>%
  ungroup() %>%
  mutate(muni = str_to_upper(muni_ror),
         muni = str_replace_all(muni, "/", " "),
         muni = str_remove_all(muni, " CENTRE")) 

# Cleaning data to remove lines with empty values for municipalities
location_values_clean <- location_values_all %>%
  filter(!is.na(muni_ror) & muni_ror != "")
# Merging with the GADM matches
merged_data <- location_values_clean %>%
  left_join(select(muni_matches_updated, -muni), 
            by = c("obs_code", "muni_ror"))

```



```{r}
# Checking for duplicated rows in location_values_clean
duplicated_rows_location_values_clean <- location_values_clean %>%
  filter(duplicated(select(., obs_code, muni_ror, hamlet_code, hamlet)) | 
           duplicated(select(., obs_code, muni_ror, hamlet_code, hamlet), fromLast = TRUE))

# Checking for duplicated rows in muni_matches_updated
duplicated_rows_muni_matches_updated <- muni_matches_updated %>%
  filter(duplicated(select(., obs_code, muni_ror, muni_code, hamlet_code, hamlet, site_code)) |
           duplicated(select(., obs_code, muni_ror, muni_code, hamlet_code, hamlet, site_code), fromLast = TRUE))

```


## Counting data per observatory



```{r}
# Define a function to load and count surveys per observatory for a given year
load_and_count <- function(year) {
  # Define file path
  file_path <- paste0("enter/", year, "/res_deb.dta")
  
  # Load data
  data <- read_dta(file_path)
  
  # Count surveys per observatory
  count_data <- data %>%
    group_by(j0) %>%
    summarise(survey_count = n()) %>%
    ungroup() %>%
    mutate(year = year)  # Add year column
  
  return(count_data)
}

# Generate a list of years
years <- 1995:2015

# Use purrr::map_df to loop through each year and bind results
result <- map_df(years, load_and_count)

result_wide <- result %>%
  pivot_wider(names_from = year, values_from = survey_count)

# Print the result
print(result)
```



## Create a data dictionnary

```{r}
# Function to extract variable info for a given year and file
extract_variable_info <- function(year, file) {
  # Define file path
  file_path <- paste0("enter/", year, "/", file)
  
  # Check if the file exists
  if (!file.exists(file_path)) return(NULL)
  
  # Load data (but only headers, not full data to speed up processing)
  data <- read_dta(file_path, n_max = 0)
  
  # Extract variable names and labels
  var_names <- names(data)
  var_labels <- var_label(data)
  
  # Create a tibble to store extracted data
  tibble(
    file_name = file,
    variable_name = var_names,
    variable_label = var_labels,
    year = year
  )
}

# Use purrr::map_df to loop through each year, list its files, and extract info
all_vars <- map_df(years, function(y) {
  # List files for the specific year
  files_for_year <- list.files(paste0("enter/", y), pattern = "\\.dta$", full.names = FALSE)
  # Apply extract_variable_info function for each file of the year
  map_df(files_for_year, extract_variable_info, year = y)
})

# Convert any NULL values in variable_label to "NA"
all_vars$variable_label <- as.character(all_vars$variable_label)
all_vars$variable_label[is.na(all_vars$variable_label)] <- "NA"

# Consolidate the information
variable_dictionary <- all_vars %>%
  group_by(file_name, variable_name) %>%
  arrange(year) %>%  # Arranging by year can ensure that we capture the most frequent or earliest label
  summarise(
    variable_label = first(variable_label[variable_label != "NA"] %||% "NA"),
    years_present = list(unique(year))
  ) %>%
  ungroup()


# Print the variable dictionary
print(variable_dictionary)
# Convert the list column to a character column
variable_dictionary$years_present <- sapply(variable_dictionary$years_present, paste, collapse = ",")

# Now split and save the chunks to CSV as before:
num_rows <- nrow(variable_dictionary)
chunk_size <- 100

split_data <- split(variable_dictionary, ceiling(seq_len(num_rows)/chunk_size))

for(i in seq_along(split_data)) {
  utils::write.table(split_data[[i]], file = paste0("chunk_", i, ".csv"), row.names = FALSE, sep = ",", quote = TRUE)
}

```


