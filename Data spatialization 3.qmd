---
title: "Appendix: data spatialization"
author: "Florent Bédécarrats"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

## Cleanig and harmonizing the municipality

### Preparing the Data for Municipality Matching

To accurately merge datasets with varying administrative names, we often have to utilize a combination of automated processes and manual review. Our approach consists of the following major steps:

1. **Reading and Preprocessing Survey Data:** Initial handling of raw survey data to extract and standardize municipality names.
2. **Processing GADM Data:** GADM offers a comprehensive spatial dataset, which we leverage for authoritative municipality names.
3. **Fuzzy Matching:** Given the potential discrepancies between names in different datasets, a fuzzy string matching technique is applied to assess the degree in which the municipality names found in the ROR survey differ from their closest match in GADM dataset.
4. **Computing Relative Distance and Preparing for Manual Review:** The relative difference between matched strings is used to guide our manual review process.

```{r}
# Load required libraries
library(tidyverse)
library(haven)
library(labelled)
library(geodata)
library(sf)
library(stringdist)
```

## Extract a list of location values

```{r}
# Define path to the survey location data
deb_files <- list.files(path = "enter", pattern = "res_deb", 
                        recursive = TRUE, 
                        full.names = TRUE) %>%
  str_subset("stunicode", negate = TRUE) 

# Define variables related to municipalty locations
location_variables <- list(
  year = "data collection year",
  j0 = "observatory code",
  obs = "observatory",
  j41 = "municipality code",
  j42 = "municipality name",
  j4_code = "village code",
  j4 = "village name",
  code_site = "site code")

# Extract location-related data from the survey files and preprocess
location_values <- deb_files %>%
  map(~ read_dta(.x) %>%
        mutate(across(where(is.labelled), ~ as.character(as_factor(.))))) %>%
  map(~ select(.x, any_of(names(location_vars_muni)))) %>%
  map(~ .x %>% mutate_all(as.character)) %>%
  map(unique) %>%
  bind_rows() %>%
  rename(obs_code = j0, muni_code = j41, muni_ror = j42) %>%
  mutate(muni = str_to_upper(muni_ror),
         muni = str_replace_all(muni, "/", " "),
         muni = str_remove_all(muni, " CENTRE")) %>%
  unique()
```


As a resukts we have a list of all variation of location variables found in the data. We have a total of `r nrow(location_values)` different commune names.

### Processing GADM Data

The GADM (Global Administrative Areas) database provides up-to-date administrative boundaries and names for all countries globally. Here, we extract the municipal names for Madagascar. These serve as our reference set in the subsequent matching process.

```{r}
munis_gadm <- gadm("MDG", level  = 4, path = "data") %>%
  st_as_sf() %>%
  mutate(muni = str_to_upper(NAME_4))
```


### Fuzzy Matching Process

Given that different datasets might have slight variations in naming conventions or typographical differences, a simple direct match can often overlook correct pairings. By employing a fuzzy string matching method, we can capture these near-matches more effectively. Here, we compute a distance matrix to identify the closest possible matches between our two sets of municipality names.

```{r fuzzy_matching}
# Compute the distance matrix between municipality names in the survey data and GADM data
distance_matrix <- stringdistmatrix(location_values$muni, munis_gadm$muni, 
                                    method = "lv")

# Identify closest matches based on the distance matrix
closest_matches <- apply(distance_matrix, 1, which.min) %>%
  as.numeric()
proximity_grades_abs <- apply(distance_matrix, 1, min)
proximity_grades_rel <- proximity_grades_abs / str_length(location_values$muni) * 100

# Bind the municipality names, their closest matches, and the proximity grades into a tibble
muni_matches <- tibble(
  muni_ror = location_values$muni_ror,
  muni = location_values$muni,
  closest_gadm_name = munis_gadm$muni[closest_matches],
  closest_gadm_district = munis_gadm$NAME_3[closest_matches],
  closest_gadm_region = munis_gadm$NAME_2[closest_matches],
  proximity_grade_abs = proximity_grades_abs,
  proximity_grade_rel = proximity_grades_rel) %>%
  arrange(desc(proximity_grade_rel)) # Order by relative distance

# Export the dataset for manual review
writexl::write_xlsx(muni_matches, "review_muni_matches.xlsx")

# Extract location-related data from the survey files and preprocess
# location_values_all <- deb_files %>%
#   map(~ read_dta(.x) %>%
#         mutate(across(where(is.labelled), ~ as.character(as_factor(.))))) %>%
#   map(~ select(.x, any_of(names(location_variables)))) %>%
#   map(~ .x %>% mutate_all(as.character)) %>%
#   map(unique) %>%
#   bind_rows() %>%
#   rename(obs_code = j0, muni_code = j41, muni_ror = j42) %>%
#   mutate(muni = str_to_upper(muni_ror),
#          muni = str_replace_all(muni, "/", " "),
#          muni = str_remove_all(muni, " CENTRE")) %>%
#   unique()
```

We computed the Levenshtein distance between each municipality name from the survey data and the GADM data. Based on this distance, we identify the closest match from the GADM dataset for each survey municipality name. The proximity grade is calculated to guide potential manual review.




To disambiguate some location names, we also use the Populated places database from the OCHA (https://data.humdata.org/fr/dataset/madagascar-settlements)




Attempt on 2007
```{r}

manual_matches <- tibble::tribble(
                     ~muni_ror,     ~manual_match,              ~source_match,
          "FERAMANGA-AVARATRA",     "AMBANDRIKA",      "OCHA located places",
          "FERAMANGA AVARATRA",     "AMBANDRIKA",      "OCHA located places",
                   "AMBODIADY",   "AMBODIFARIHY",                   "Visual",
                 "ANDROKAVATO",         "AMBANO", "agritrop.cirad.fr/558679",
"VOHITROMBY VOHITROMBY CENTRE",     "VOHITROMBY",                   "Visual",
   "AMBATOHARANANA MAHANTSARA", "AMBATOHARANANA",                   "Visual",
 "AMBATOHARANANA AMBODIBARIKA", "AMBATOHARANANA",                   "Visual",
                    "BEZEZIKA",     "ANKILIVALO",      "OCHA located places",
                    "BEKININY",         "BEFASY",      "OCHA located places",
                "FORT DAUPHIN",       "TOLANARO",                "Wikipedia",
              "TSIVORY CENTRE",        "TSIVORY",                   "Visual")

# We use the automatic match or manual one when we created one
muni_names_matches_2007 <- muni_names_matches_2007 %>%
  left_join(select(manual_matches, muni_ror, manual_match),
            by = join_by("muni_ror")) %>%
  mutate(gadm_match = ifelse(is.na(manual_match), 
                             closest_gadm_name, manual_match)) %>%
  select(muni_ror, gadm_match) %>%
  left_join()

# Get the original names back into loc_2007
loc_2007_2 <- muni_names_matches_2007 %>%
  left_join(munis_gadm %>%
              st_drop_geometry() %>%
              select(gadm_match = muni, name_gadm = NAME_4),
            by = join_by("gadm_match")) 


munis_gadm %>%
  st_drop_geometry() %>%
  group_by(NAME_1, NAME_4) %>%
  summarize(count = n()) %>%
  filter(count > 1)



# Andrakavato est un hameau de la commune d'Ambano
# Source: https://agritrop.cirad.fr/558679/1/document_558679.pdf
```


```{r}
# Define a function to load and count surveys per observatory for a given year
load_and_count <- function(year) {
  # Define file path
  file_path <- paste0("enter/", year, "/res_deb.dta")
  
  # Load data
  data <- read_dta(file_path)
  
  # Count surveys per observatory
  count_data <- data %>%
    group_by(j0) %>%
    summarise(survey_count = n()) %>%
    ungroup() %>%
    mutate(year = year)  # Add year column
  
  return(count_data)
}

# Generate a list of years
years <- 1995:2015

# Use purrr::map_df to loop through each year and bind results
result <- map_df(years, load_and_count)

result_wide <- result %>%
  pivot_wider(names_from = year, values_from = survey_count)

# Print the result
print(result)
```



## Create a data dictionnary

```{r}
# Function to extract variable info for a given year and file
extract_variable_info <- function(year, file) {
  # Define file path
  file_path <- paste0("enter/", year, "/", file)
  
  # Check if the file exists
  if (!file.exists(file_path)) return(NULL)
  
  # Load data (but only headers, not full data to speed up processing)
  data <- read_dta(file_path, n_max = 0)
  
  # Extract variable names and labels
  var_names <- names(data)
  var_labels <- var_label(data)
  
  # Create a tibble to store extracted data
  tibble(
    file_name = file,
    variable_name = var_names,
    variable_label = var_labels,
    year = year
  )
}

# Use purrr::map_df to loop through each year, list its files, and extract info
all_vars <- map_df(years, function(y) {
  # List files for the specific year
  files_for_year <- list.files(paste0("enter/", y), pattern = "\\.dta$", full.names = FALSE)
  # Apply extract_variable_info function for each file of the year
  map_df(files_for_year, extract_variable_info, year = y)
})

# Convert any NULL values in variable_label to "NA"
all_vars$variable_label <- as.character(all_vars$variable_label)
all_vars$variable_label[is.na(all_vars$variable_label)] <- "NA"

# Consolidate the information
variable_dictionary <- all_vars %>%
  group_by(file_name, variable_name) %>%
  arrange(year) %>%  # Arranging by year can ensure that we capture the most frequent or earliest label
  summarise(
    variable_label = first(variable_label[variable_label != "NA"] %||% "NA"),
    years_present = list(unique(year))
  ) %>%
  ungroup()


# Print the variable dictionary
print(variable_dictionary)
# Convert the list column to a character column
variable_dictionary$years_present <- sapply(variable_dictionary$years_present, paste, collapse = ",")

# Now split and save the chunks to CSV as before:
num_rows <- nrow(variable_dictionary)
chunk_size <- 100

split_data <- split(variable_dictionary, ceiling(seq_len(num_rows)/chunk_size))

for(i in seq_along(split_data)) {
  utils::write.table(split_data[[i]], file = paste0("chunk_", i, ".csv"), row.names = FALSE, sep = ",", quote = TRUE)
}

```


