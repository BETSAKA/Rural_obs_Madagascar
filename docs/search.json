[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Madagascar rural observatory system data validation, preparation, and georeferencing",
    "section": "",
    "text": "Introduction\nThis technical appendix is a companion to a data descriptor article, submitted to Nature Scientific Data, that focuses on data collected by the Rural Observatory System (ROS) between 1995 and 2015. It provides the source code for all figures and visualizations presented in the paper. It also offers a tutorial on how to georeference this data, which can serve as guidance for various types of analysis beyond this specific application. We use computational notebooks in Quarto format with the R programming language, combining code, results, explanations, and multimedia in an interactive way. The source code can be accessed by expanding code blocks like the following one, which produced the figure below.\n\n\nCode\n# Load required libraries\nlibrary(tidyverse)    # A series of packages for data manipulation\nlibrary(haven)        # Required for reading STATA files (.dta)\nlibrary(labelled)     # To work with labelled data from STATA\nlibrary(sf)           # for spatial data handling\nlibrary(tmap)         # for mapping\nlibrary(readxl)       # Read data frames to Excel format\nlibrary(cowplot)      # to combine plots\n\n# Select appropriate folder as data source\ndata_path &lt;- \"data/dta_format/\"\n\n# Define a function to load and count surveys per observatory for a given year\nload_and_count &lt;- function(year, factorize = FALSE) {\n  # Define file path\n  file_path &lt;- paste0(data_path, year, \"/res_deb.dta\")\n  \n  # Load data\n  data &lt;- read_dta(file_path)\n  \n  # Extract label and convert to factors if option\n  if (factorize) {\n    data &lt;- data %&gt;%\n      mutate(across(everything(), as.character),\n             across(where(is.labelled), ~ as.character(as_factor(.))))\n  }\n  \n  # Count surveys per observatory\n  count_data &lt;- data %&gt;%\n    group_by(j0) %&gt;%\n    summarise(survey_count = n()) %&gt;%\n    ungroup() %&gt;%\n    mutate(year = year)  # Add year column\n  \n  return(count_data)\n}\n\n# Generate a list of years\nyears &lt;- 1995:2015\n\n# Use purrr::map_df to loop through each year and bind results\nobs_count &lt;- map_df(years, load_and_count) %&gt;%\n  # Remove rows with observatory \"7 \" and \"NA\", which are errors\n  filter(j0 != 7 & !is.na(j0) & survey_count &gt; 1) %&gt;%\n  rename(observatory = j0)\n\n# Read observatory names\nobservatory_names &lt;- readxl::read_xlsx(\"references/observatory_names.xlsx\") %&gt;%\n  select(code, observatory_name = name)\n\n# PAss it to wide.\nobs_count &lt;- obs_count %&gt;%\n  left_join(observatory_names, by = c(\"observatory\" = \"code\")) %&gt;%\n  group_by(observatory_name, year) %&gt;%\n  summarise(survey_count = sum(survey_count))\n\nobs_count_wide &lt;- obs_count %&gt;%\n  pivot_wider(names_from = year, values_from = survey_count)\n\n# Add observatory approximate location\nlocations &lt;- tibble(\n  code = c(1, 2, 3, 4, 12, 13, 15, 16, 21, 22, 23, 24, 31, 25, 41, 42, 43, 51, \n           44, 45, 61, 17, 18, 19, 71, 52),\n  name = c(\"Antalaha\", \"Antsirabe\", \"Marovoay\", \"Toliara coastal\", \"Antsohihy\", \n           \"Tsiroanomandidy\", \"Farafangana\", \"Ambovombe\", \n           \"Alaotra\", \"Manjakandriana\", \"Toliara North\", \n           \"Fenerive East\", \"Bekily\", \"Mahanoro\", \"Itasy\", \n           \"Menabe-Belo\", \"Fianarantsoa\", \"Tsivory\", \"Morondava\", \"Manandriana\", \n           \"Tanandava\", \"Ihosy\", \"Ambohimahasoa\", \"Manakara\", \"Tolanaro\", \n           \"Menabe North-East\"),\n  latitude = c(-14.8833, -19.8659, -16.1000, -23.7574, -14.8796, -18.7713, \n               -22.8167, -25.1667, -17.8319, -18.9167, -23.2941, -17.3500, \n               -24.6900, -19.9000, -19.1686, -19.6975, -21.4527, -24.4667, \n               -20.2833, -20.2333, -22.5711, -22.4000, -20.7145, -22.1333, \n               -25.0381, -20.5486),\n  longitude = c(50.2833, 47.0333, 46.6333, 43.6770, 47.9875, 46.0546, 47.8333, \n                46.0833, 48.4167, 47.8000, 43.7761, 49.4167, 45.1700, 48.8000,\n                46.7354, 44.5419, 47.0857, 45.4667, 44.2833, 47.3833, 45.0439, \n                46.1167, 47.0389, 48.0167, 46.9562, 47.1597))\n\nobs_count &lt;- left_join(obs_count, locations, by = c(\"observatory_name\" = \"name\"))\n\n\nmadagascar &lt;- st_read(paste0(\"data/Spatial_data/OCHA_BNGRC admin boundaries/\",\n                             \"mdg_admbnda_adm0_BNGRC_OCHA_20181031.shp\"),\n                      quiet = TRUE)\n\n# Sort locations by latitude to generate sequence numbers\nlocations &lt;- locations %&gt;%\n  arrange(desc(latitude)) %&gt;%\n  mutate(seq_num = 1:n())\n\n# Create map plot with labels\nmap_plot &lt;- ggplot(data = madagascar) +\n  geom_sf(fill = \"lightgray\", colour = \"dimgrey\") +\n  geom_point(data = locations, aes(x = longitude, y = latitude, color = name), \n             size = 3) +\n  geom_text(data = locations, aes(x = longitude, y = latitude, label = seq_num), \n            vjust = -1, hjust = 1, size = 3, # check_overlap = TRUE,\n            fontface = \"bold\") + \n  theme_void() +\n  theme(legend.position = \"none\")\n\n# Add sequence numbers to observatory names in obs_count dataframe\nobs_count &lt;- obs_count %&gt;%\n  left_join(locations %&gt;%\n              select(name, seq_num), \n            by = c(\"observatory_name\" = \"name\")) %&gt;%\n  mutate(observatory_with_num = paste0(seq_num, \". \", observatory_name))\n\n# Create timeline plot using modified obs_count with observatory_with_num\ntimeline_plot &lt;- ggplot(obs_count, \n                        aes(x = year, \n                            y = fct_reorder(observatory_with_num, latitude), \n                            color = observatory_name)) +\n  geom_point(aes(size = 1), show.legend = F) +\n  theme_minimal() +\n  labs(y = NULL, x = NULL) +\n  theme(axis.text.y = element_text(size = 8, face = \"bold\"),\n        legend.position = \"none\")\n\n# Stitch the plots together\ncombined_plot &lt;- plot_grid(map_plot, timeline_plot, rel_widths = c(1.3, 2))\n\nggsave(\"output/figure_1.pdf\", plot = combined_plot, \n       width = 10, height = 7, dpi = 300)\nggsave(\"output/figure_1.png\", plot = combined_plot, \n       width = 10, height = 7, dpi = 300)\n\nprint(combined_plot)\n\n\n\n\n\nFigure 1: Coarse location of rural observatory and survey years\n\n\n\n\nFigure 1 shows the coarse location of the 26 observatories composing the ROS, as well as the years in which data was collected in each one."
  },
  {
    "objectID": "01-ros-data-catalogue.html",
    "href": "01-ros-data-catalogue.html",
    "title": "1  Data catalog",
    "section": "",
    "text": "The ROS survey data is organized in a collection of year-specific folders ranging from 1995 to 2015. Each yearly folder houses multiple .dta files (Stata data format) – about 85 per year – with diverse filenames such as “res_as.dta” and “res_bp.dta”. The code block below creates a data dictionary, which can be downloaded by clicking on this link. It also displays the interactive table below, which can be browsed or searched.\n\n\nCode\nlibrary(tidyverse)    # A series of packages for data manipulation\nlibrary(haven)        # Required for reading STATA files (.dta)\nlibrary(labelled)     # To work with labelled data from STATA\nlibrary(writexl)      # Write data frames to Excel format\nlibrary(readxl)\n\nros_data_loc &lt;- \"data/dta_format/\"\n\n# Function to extract variable info for a given year and file\nextract_variable_info &lt;- function(year, file) {\n  \n  file_path &lt;- paste0(ros_data_loc, year, \"/\", file)\n  \n  if (!file.exists(file_path)) return(tibble())\n  \n  data &lt;- read_dta(file_path, n_max = 0)\n  \n  tibble(\n    file_name = file,\n    variable_name = names(data),\n    variable_label = var_label(data) %&gt;% as.character(),\n    year = year)\n}\n\n# Obtain all years from the directory structure\nyears &lt;- list.dirs(ros_data_loc, recursive = FALSE, full.names = FALSE)\n\n# Use the tidyverse approach to map over years and files\nall_vars &lt;- map_df(years, ~{\n  files_for_year &lt;- list.files(paste0(ros_data_loc, .x), pattern = \"\\\\.dta$\", full.names = FALSE)\n  map_df(files_for_year, extract_variable_info, year = .x)\n})\n\n# Convert any NULL values in variable_label to \"NA\"\nall_vars$variable_label[is.na(all_vars$variable_label)] &lt;- \"NA\"\n\n# retreive the file labels\nfile_labels &lt;- read_excel(\"references/File_labels.xlsx\") %&gt;%\n  select(file_name = filename, file_label = title_en)\n\n# Consolidate the information using the tidyverse approach\nvariable_dictionary &lt;- all_vars %&gt;%\n  group_by(file_name, variable_name) %&gt;%\n  arrange(year) %&gt;%  \n  summarise(\n    variable_label = first(variable_label[variable_label != \"NA\"] %||% \"NA\"),\n    years_present = list(unique(year))) %&gt;%\n  ungroup() %&gt;%\n  mutate(years_present = map_chr(years_present, ~ paste(.x, collapse = \",\"))) %&gt;%\n  left_join(file_labels, by = \"file_name\", .before) %&gt;%\n  relocate(file_label, .after = file_name) %&gt;%\n  arrange(substr(years_present, 1, 4), # To have 1st variables of 1995\n          case_when(file_name == \"res_deb.dta\" ~ as.integer(1),\n                    file_name == \"res_h.dta\" ~ as.integer(2),\n                    TRUE ~ as.integer(3))) # starts with hh ID and housing\n\n# Write the variable dictionary to an Excel file\nwrite_xlsx(variable_dictionary, \"output/ROS_Variable_Dictionary.xlsx\")\n# To include in published dataset\nwrite_xlsx(variable_dictionary, \"data/ROS_Variable_Dictionary.xlsx\")\n\n# Display in interactive format\nDT::datatable(variable_dictionary)"
  },
  {
    "objectID": "02-ros-data-attrition.html",
    "href": "02-ros-data-attrition.html",
    "title": "2  Panel attrition",
    "section": "",
    "text": "To assess the integrity of the panel data, we assess the attrition that characterizes it. That is, for each survey reiteration, we compute the percentage of households identified in the previous round that are still present in the subsequent round. Some adjustment must be made as households’ identification numbering system between 1995 and 1996. We also have to remove 2005 survey in Marovoay from the analysis, as it was a specific tracking survey aimed at identifying individuals from households that could not be re-interviewed in previous years in one of the observatory sites (Vaillant 2013). This produces the following result:\n\n\nCode\nlibrary(tidyverse)    # A series of packages for data manipulation\nlibrary(haven)        # Required for reading STATA files (.dta)\nlibrary(labelled)     # To work with labelled data from STATA\nlibrary(readxl)       # Read data frames to Excel format\n\n# Obtain all years from the directory structure\nros_data_loc &lt;- \"data/dta_format/\"\nyears &lt;- list.dirs(ros_data_loc, recursive = FALSE, full.names = FALSE)\n\n# Add observatory approximate location\nlocations &lt;- tibble(\n  code = c(1, 2, 3, 4, 12, 13, 15, 16, 21, 22, 23, 24, 31, 25, 41, 42, 43, 51, \n           44, 45, 61, 17, 18, 19, 71, 52),\n  name = c(\"Antalaha\", \"Antsirabe\", \"Marovoay\", \"Toliara coastal\", \"Antsohihy\", \n           \"Tsiroanomandidy\", \"Farafangana\", \"Ambovombe\", \n           \"Alaotra\", \"Manjakandriana\", \"Toliara North\", \n           \"Fenerive East\", \"Bekily\", \"Mahanoro\", \"Itasy\", \n           \"Menabe-Belo\", \"Fianarantsoa\", \"Tsivory\", \"Morondava\", \"Manandriana\", \n           \"Tanandava\", \"Ihosy\", \"Ambohimahasoa\", \"Manakara\", \"Tolanaro\", \n           \"Menabe North-East\"),\n  latitude = c(-14.8833, -19.8659, -16.1000, -23.7574, -14.8796, -18.7713, \n               -22.8167, -25.1667, -17.8319, -18.9167, -23.2941, -17.3500, \n               -24.6900, -19.9000, -19.1686, -19.6975, -21.4527, -24.4667, \n               -20.2833, -20.2333, -22.5711, -22.4000, -20.7145, -22.1333, \n               -25.0381, -20.5486),\n  longitude = c(50.2833, 47.0333, 46.6333, 43.6770, 47.9875, 46.0546, 47.8333, \n                46.0833, 48.4167, 47.8000, 43.7761, 49.4167, 45.1700, 48.8000,\n                46.7354, 44.5419, 47.0857, 45.4667, 44.2833, 47.3833, 45.0439, \n                46.1167, 47.0389, 48.0167, 46.9562, 47.1597))\n\n# Sort locations by latitude to generate sequence numbers\nlocations &lt;- locations %&gt;%\n  arrange(desc(latitude)) %&gt;%\n  mutate(seq_num = 1:n())\n\n\n# Function to read and process each file\nread_and_process &lt;- function(year) {\n  file_path &lt;- file.path(ros_data_loc, as.character(year), \"res_deb.dta\")\n  data &lt;- read_dta(file_path) %&gt;%\n    select(j0, j5) %&gt;%\n    mutate(year = year)\n  return(data)\n}\n\n# Use map to read and process files, then combine with bind_rows\nconsolidated_data &lt;- map_dfr(years, read_and_process) %&gt;%\n  mutate(year = as.numeric(year))\n\n# NB : j5 codes have been modified in 1996\n# so we need to replace the ones from 1995\nhh_96 &lt;- read_dta(paste0(ros_data_loc, \"1996/res_deb.dta\")) %&gt;%\n  select(j0, year, j5_96 = j5, j_1995, j12b) %&gt;%\n  filter(j_1995 == 1) %&gt;%\n  select(-j_1995) %&gt;%\n  mutate(year = 1995) %&gt;%\n  distinct(j12b, .keep_all = TRUE)\n\nconsolidated_data &lt;- consolidated_data %&gt;%\n  left_join(hh_96, by = c(\"j0\", \"year\", \"j5\" = \"j12b\")) %&gt;%\n  mutate(j5 = ifelse(year == 1995 & !is.na(j5_96), j5_96, j5)) %&gt;%\n  select(j0, j5, year)\n\n# We need also to discard the 2004 survey in Marovoay that is very particular\n# cf. Vaillant 2013.\nconsolidated_data &lt;- consolidated_data %&gt;%\n  filter(!(j0 == 3 & year == 2005))\n\n# Remove duplicates and create the hh_all table\nhh_all &lt;- consolidated_data %&gt;%\n  distinct(j0, j5, year, .keep_all = TRUE) %&gt;%\n  arrange(j0, j5)\n\nhh_grouped &lt;- hh_all %&gt;%\n  group_by(j0, year) %&gt;%\n  summarise(j5_list = list(j5), .groups = 'drop') %&gt;%\n  # Count the number of j5 in j5_list\n  mutate(j5_count = map_int(j5_list, length)) %&gt;%\n  # Create a column to identify the most recent previous year with data for the same observatory\n  group_by(j0) %&gt;%\n  mutate(previous_year = lag(year)) %&gt;%\n  ungroup()\n  \n# Self-join to create previous_year_j5_list\nattrition_rates_detail &lt;- hh_grouped %&gt;%\n  left_join(hh_grouped %&gt;% select(j0, year, \n                                  previous_year_j5_list = j5_list,\n                                  j5_count_previous_year = j5_count), \n            by = c(\"j0\", \"previous_year\" = \"year\")) %&gt;%\n  mutate(repeated_j5 = map_int(\n    seq_along(j5_list), \n    ~length(intersect(j5_list[[.]], previous_year_j5_list[[.]]))),\n    attrition_rate = (j5_count_previous_year - repeated_j5) /\n      j5_count_previous_year * 100)\n\n\n# Pivot the data to have years as columns and observatory numbers as rows\nattrition_rates &lt;- attrition_rates_detail  %&gt;%\n  select(j0, year, attrition_rate) %&gt;%\n  left_join(locations %&gt;%\n              mutate(observatory_with_num = paste0(seq_num, \". \", name),\n                     observatory_with_num = fct_reorder(observatory_with_num, \n                                                        latitude)) %&gt;%\n              select(code, name, observatory_with_num), \n            by = c(\"j0\" = \"code\")) %&gt;%\n  drop_na(name)\n\naverage_wo_outliers &lt;- attrition_rates %&gt;%\n  filter(attrition_rate &lt; 75) %&gt;%\n  summarise(mean = mean(attrition_rate))\n\naverage_wo_outliers &lt;- round(average_wo_outliers$mean, 1)\ncompound_avg_10y &lt;- round((1-(1-(average_wo_outliers/100))^10)*100) \n\nattrition_plot &lt;- ggplot(attrition_rates, \n                         aes(x = year, y = observatory_with_num, \n                             fill = attrition_rate)) +\n  geom_tile() +  # Create the heatmap tiles\n  geom_text(aes(label = ifelse(is.na(attrition_rate), \"\", \n                               round(attrition_rate))), \n            color = \"black\", size = 2.5) +\n  scale_fill_gradient2(low = \"darkgreen\", mid = \"yellow\", high = \"red\", \n                       midpoint = 30, na.value = \"grey\", name = \"Attrition Rate (%)\") +\n  labs(x = \"Year\",\n       y = \"Observatory (j0)\") +\n  theme_minimal() +\n  labs(y = NULL, x = NULL) +\n  theme(axis.text.y = element_text(size = 8)) \n\nggsave(\"output/figure_3.pdf\", plot = attrition_plot, \n       width = 7, height = 4, dpi = 300)\nggsave(\"output/figure_3.png\", plot = attrition_plot, \n       width = 7, height = 4, dpi = 300)\nattrition_plot\n\n\n\n\n\nFigure 2.1: Attrition rate of ROS panels per observatory and survey round\n\n\n\n\nAnnual attrition rates superior to 75% for a specific observatory are likely to be induced by new reshuffles of the household identification codes and we hope to be able to solve such issue later on. If we discard these outliers (attrition rates over 75%), we have an average attrition year of {r} print(average_wo_outliers)%, which is very high, leading to a compound attrition rate of {r} print(compound_avg_10y)% over 10 years. Attrition on ROS data has been further studied in focused publications (Gubert and Robilliard 2008; Vaillant 2013).\n\n\n\n\nGubert, Flore, and Anne-Sophie Robilliard. 2008. “Risk and Schooling Decisions in Rural Madagascar: A Panel Data-Analysis.” Journal of African Economies 17 (2): 207–38. https://doi.org/10.1093/jae/ejm010.\n\n\nVaillant, Julia. 2013. “Attrition and Follow-Up Rules in Panel Surveys: Insights from a Tracking Experience in Madagascar.” Review of Income and Wealth 59 (3): 509–38. https://doi.org/10.1111/j.1475-4991.2012.00505.x."
  },
  {
    "objectID": "03-ros-data-georeferencing.html#shared-geographic-reference-system",
    "href": "03-ros-data-georeferencing.html#shared-geographic-reference-system",
    "title": "3  Georeferencing",
    "section": "3.1 Shared geographic reference system",
    "text": "3.1 Shared geographic reference system\nThe main purpose of this section is to enhance the georeferencing of the ROS survey data for open data sharing. The initial ROS survey, initiated in 1995, recorded geographical information in varying formats: from “village” to a combination of “municipality”, “village”, and “site”. A significant challenge was that data collection started in 1995, while in Madagascar municipalities were only formally established in 1994, and it took several years required for stabilization. The toponyms, which mostly came from oral traditions, were subjected to change, resulting in varied written representations. Our goal is to identify, disambiguate, and georeference the observations recorded in the ROS data, adopting the Common Operational Datasets (CODs) as a reference, which have been collaboratively defined by OCHA and Madagascar’s BNGRC (National Disaster Management Office).\n\n3.1.1 The Common Operational Dataset\nCODs are the foundation for all preparedness and response operations, particularly within the humanitarian sector. They were adopted by the IASC in 2008 and revised in 2010, and they play a crucial role in facilitating informed decision-making during the critical initial hours of a crisis. By ensuring consistency among stakeholders, CODs simplify data management and establish a shared operational picture of a crisis. Of particular relevance to our purpose is the inclusion of P-codes in CODs. These unique geographic identification codes can be found in both Administrative Boundary CODs (COD-ABs) and Population Statistics CODs (COD-PSs), and they help overcome challenges posed by variations in place names and spellings. For example, in Madagascar, there are 81 different administrative level 4 (ADM4) features labeled “Morafeno,” with six of them existing within ADM3 features also called “Morafeno.” The only way to distinguish them is through their unique ADM2 features.\nP-codes serve as reliable geographic identifiers, eliminating errors that may arise from identical or differently spelled geographic locations. Using the HDX platform, an open platform for cross-crisis data sharing, we retrieve this data to ensure the accurate georeferencing of our ROS data. By leveraging the standardized and official spelling of places provided by P-codes, we can combine, harmonize, and analyze data from various sources, thereby offering a comprehensive and geographically accurate view of the survey’s findings.\nAlthough some level of harmonization is achieved, especially regarding certain variables and household identifiers, the data varies in terms of geographical granularity. In the early years, the data primarily includes a single field indicating the village name. As the years go by, this evolves to include a municipality name, and in later years, an additional “site” name occasionally appears. A comprehensive overview of the observations can be obtained from the “res_deb.dta” files for each year.\n\n\n3.1.2 Administrative boundaries\nThe “Madagascar Subnational Administrative Boundaries” dataset is sourced from the Common Operational Datasets (CODs), which provide authoritative reference datasets for decision-making during humanitarian operations. CODs are specifically designed to streamline the discovery and exchange of essential data, ensuring uniformity and using the ‘best available’ datasets. This particular dataset focuses on administrative boundaries, including gazetteers with P-codes, which facilitate organized humanitarian assessments and data management. P-codes act as unique identifiers for every administrative unit and populated area, ensuring standardization in nomenclature. When datasets adhere to the P-code standard, their integration and analysis become more efficient. The dataset provides comprehensive boundary information for Madagascar at five administrative levels: country, region, district, commune, and fokontany. It is accessible for download as shapefiles from the provided link.\n\n\n3.1.3 Localities\nThe “Madagascar Populated Places” dataset is also part of the Common Operational Datasets (CODs). This dataset includes populated place points for Madagascar. The data has been sourced from the National Geospatial-Intelligence Agency and provided by the University of Georgia - ITOS. Furthermore, the Geographic Information Support Team (GIST) has taken on the role of distributor, with the data being published on 2007-03-07. UN OCHA ROSA has enhanced the dataset by adding P-codes and administrative boundary names based on the BNGRC (National Disaster Management Office) data. The dataset geolocates 28,184 populated places with their toponyms (names), codes related to various administrative levels such as fokontany, commune, district, and region, as well as their spatial coordinates.\n\n\nCode\n# Load datasets\nROS_surveys_2007 &lt;- read_dta(paste0(ros_data_loc, \"2007/res_deb.dta\"))\nobservatory_names &lt;- read_xlsx(\"references/observatory_names.xlsx\")\nobs_communes &lt;- st_read(\n  \"data/Spatial_data/Observatoires_ROS_communes_COD_v3.gpkg\",\n  quiet = TRUE) %&gt;%\n  left_join(select(observatory_names, name, code), \n            by = c(\"OBS_NAME\" = \"name\")) %&gt;%\n  rename(OBS_CODE = code)\npop_places &lt;- st_read(\n  \"data/Spatial_data/OCHA_BNGRC populated places/mdg_pplp_places_NGA_OCHA.shp\", quiet = TRUE)"
  },
  {
    "objectID": "03-ros-data-georeferencing.html#georeferencing-methodology",
    "href": "03-ros-data-georeferencing.html#georeferencing-methodology",
    "title": "3  Georeferencing",
    "section": "3.2 Georeferencing methodology",
    "text": "3.2 Georeferencing methodology\n\n3.2.1 Simplify strings\nThe treatment of toponyms presents a unique challenge, especially when these names are captured from varied sources. In the dataset, these names can vary due to differences in languages, case sensitivity, and the inclusion of additional descriptive terms. To address this, the clean_string function was developed. This function begins by converting all strings to lowercase, ensuring that subsequent comparisons are not sensitive to case variations. Next, to create a uniform standard, all non-alphanumeric characters are removed, retaining only spaces and the alphanumeric content. Common qualifiers in toponyms, such as “centre”, “haut” (high) or “bas” (low), which are not consistently used across records, are also removed. Given the bilingual nature of the dataset, with entries potentially in both Malagasy and French, the function translates cardinal points to the Malagasy language to ensure uniformity. Lastly, certain locales with multiple names, such as “Fort Dauphin”, also known as “Taolagnaro”, “Tolagnaro” or “Tolanaro”, are standardized to a single term, “Tolanaro”, to eliminate potential disparities. We also address instances of Roman numerals from I to VI, converting them to their Arabic numeral counterparts, ensuring consistent representation across records.\n\n\nCode\nclean_string &lt;- function(x){\n  x %&gt;%\n    tolower() %&gt;% # Convert to lowercase\n    # Retain spaces, remove other non-alphanumeric characters\n    str_replace_all(\"[^[:alnum:][:space:]]\", \" \") %&gt;% \n    str_remove_all(\"\\\\b(centre|haut|bas|androy)\\\\b\") %&gt;%\n    str_trim() %&gt;% # Trim spaces from start and end of string\n    str_replace_all(\"\\\\bcentre\\\\b\", \"\") %&gt;% # Remove the word 'centre'\n    # Translate cardinal points\n    str_replace_all(\"\\\\bnord\\\\b\", \"avaratra\") %&gt;% \n    str_replace_all(\"\\\\best\\\\b\", \"atsinanana\") %&gt;%\n    str_replace_all(\"\\\\bouest\\\\b\", \"andrefana\") %&gt;% \n    str_replace_all(\"\\\\bsud\\\\b\", \"atsimo\") %&gt;% \n    str_replace_all(\"\\\\batsinana\\\\b\", \"atsinanana\") %&gt;% # Replace short form \n    str_replace_all(\"(fort dauphin)|(taolagnaro)|(tolagnaro)\", \n                    \"tolanaro\") %&gt;% # Variations for fort dauphin\n    # Convert Roman numerals to Arabic\n    str_replace_all(\"\\\\bi\\\\b\", \"1\") %&gt;% \n    str_replace_all(\"\\\\bii\\\\b\", \"2\") %&gt;%\n    str_replace_all(\"\\\\biii\\\\b\", \"3\") %&gt;% \n    str_replace_all(\"\\\\biv\\\\b\", \"4\") %&gt;% \n    str_replace_all(\"\\\\bv\\\\b\", \"5\") %&gt;% \n    str_replace_all(\"\\\\bvi\\\\b\", \"6\")\n}\n\n\n\n\n3.2.2 Fuzzy matching\nBy default, statistical softwares and computing languages match text by pairing only identical strings. Exact string matching is inappropriate in our context, where location data entry was subject to human errors like typographical mistakes or minor variations in spelling. To avoid this rigidity, fuzzy matching is employed. This approach gauges the degree of similarity between two strings, bypassing the need for an exact character-to-character match. The principle metric adopted for this is the “Levenshtein distance,” which quantifies the minimum number of single-character edits required to change one string into another. The fuzzy_match function encapsulates this approach. The function initiates the process by filtering the reference list of encontered toponyms based on a given observatory code, which considerably narrows down potential matches. Then, using the Jaro-Winkler distance metric — a variant of the Levenshtein distance particularly suited for shorter strings — the function computes the similarity between the target string and entries in the filtered reference. To ensure that only relevant matches are acknowledged, a threshold, termed max_distance, is set. Matches that exceed this threshold are disregarded. For those that pass this validation, the function then extracts the pertinent details of the matched row from the reference dataframe.\n\n\nCode\nfuzzy_match &lt;- function(target_string, dataframe, column_name, observatory_code, \n                        max_distance = 0.25) {\n  # Filter the dataframe based on observatory_code\n  filtered_reference &lt;- dataframe %&gt;%\n    filter(OBS_CODE == observatory_code) %&gt;%\n    select(all_of(column_name), ADM3_PCODE, ADM3_EN)\n  \n  # If filtered_reference is empty, return NA values\n  if (nrow(filtered_reference) == 0) {\n    return(list(matched_string = NA, ADM3_PCODE = NA, ADM3_EN = NA, distance = 1))\n  }\n  \n  # Use stringdist to find the closest match\n  distances &lt;- stringdist::stringdistmatrix(target_string, filtered_reference[[column_name]], method = \"jw\")\n  \n  # If there are no valid distances, set min_distance to Inf\n  if(all(is.na(distances))) {\n    min_distance &lt;- Inf\n  } else {\n    min_distance &lt;- min(distances, na.rm = TRUE)  # Ensure NA values don't affect the min calculation\n  }\n  \n  # Check for Inf distance and replace it with 1\n  if (is.infinite(min_distance)) {\n    min_distance &lt;- 1\n  }\n  \n  # If min_distance exceeds the max_distance threshold, return NA values\n  if (min_distance &gt; max_distance) {\n    return(list(matched_string = NA, ADM3_PCODE = NA, ADM3_EN = NA, distance = NA))\n  }\n  \n  matched_row &lt;- filtered_reference[which.min(distances), ]\n  \n  return(list(matched_string = matched_row[[column_name]], \n              ADM3_PCODE = matched_row$ADM3_PCODE, \n              ADM3_EN = matched_row$ADM3_EN, \n              distance = min_distance))\n}\n\n\n\n\n3.2.3 Hierarchical matching of data\nThe hierarchical organization of spatial entities is key for our challenge. Such an organization allows for a cascading representation of data, starting from broader scopes and narrowing down to more specific layers. This representation is reminiscent of the defining order of geographical entities: regions contain provinces, which contain municipalities, and these in turn contain localities. In the georeferencing context, leveraging this hierarchical structure can lead to more precise matches. For instance, if an observatory code is associated with a specific district, the search for matches is confined to that district, enhancing both the efficiency and accuracy of the process. Figure 3.1 represents this hierarchical arrangement, serving as a roadmap for the subsequent data matching tasks.\n\n\n\n\ngraph TD\n\nA[Legend]\n\nW[ ]\nX[ ]\nY[ ]\nZ[ ]\n\nW --&gt;|Pre-defined hierarchy| X\nY -.-|Implemented matching|Z\n\n\n\n\n\n\n\n\n\n\ngraph TD\n\n%% Administrative Logic\nsubgraph \"Administrative Logic\"\nA[Region]\nB[District]\nC[Commune]\nD[Fokontany]\nI[Populated places]\n\nA --&gt; B\nB --&gt; C\nC --&gt; D\nD --&gt; I\nend\n\n%% Observatory Logic\n\nsubgraph \"Observatory Logic\"\nE[Observatory system]\nF[Observatory]\nG[Commune]\nH[Village]\nJ[Site]\n\nE --&gt; F\nF --&gt; G\nG --&gt; H\nF --&gt; H\nH--&gt; J\nend\n\n%% Geospatial matching\nH -.- I\nC -.- G\n\n\nFigure 3.1: Spatial entities pre-defined relationship and matching\n\n\n\n\nMadagascar’s current administrative setup is straightforward: regions contain districts; districts have communes; and communes are made up of Fokontany. Though a Fokontany should in principle be a single village, it often includes multiple villages or populated areas. It’s worth noting that while the idea of communes has been around for a while, they were only officially recognized in 1994. However, rolling them out took some time after 1994. Before 1994, “communes” simply described local government areas without any formal administrative status. The regions were created in 2004. On the ROS side, observatory referred during the first surveys to villages. A systematic registry of the communes only started in the 2004 and 2005 rounds, depending on the observatories. A mention to “sites” also appeared in 2011 but was scarcely documented. Our strategy was to established links between the village information and populated places, and between communes."
  },
  {
    "objectID": "03-ros-data-georeferencing.html#a-detailed-walk-through",
    "href": "03-ros-data-georeferencing.html#a-detailed-walk-through",
    "title": "3  Georeferencing",
    "section": "3.3 A Detailed walk-through",
    "text": "3.3 A Detailed walk-through\nWe now break down our approach to describe each subsequent step. We began by exploring the ROS documentation, and in particular the reports associated to community survey, that contain descriptions of the area surveyed by each observatory. While doing so, we updated the COD subnational administrative boundary dataset by adding a field named OBS_Y_N. This field was marked as ‘1’ if the municipality was listed in an observatory survey; if not, it was left empty. Additionally, we added another field, OBS_NUM, which would store the observatory number. If the municipality wasn’t part of any survey, this field was left empty. The list of observatories and surveyed municipalities can be found in the ?tbl-list-muni.\nNext, we moved on to geolocation, which took place in four stages. Each stage depended heavily on the data quality and completeness obtained from the previous ones.\n\n3.3.1 Method 1\nThis phase involved a systematic alignment process for the municipalities. When municipality names were included in our dataset, we attempted to correlate them with names of municipalities identified as data collection locations. To ensure accuracy, this alignment was carried out separately for each observatory. Due to possible differences in terminology across sources, a fuzzy matching algorithm was used. This method calculates the likelihood of two different names referring to the same entity. Once a probable match was identified, we visually verified all matches and flagged any false positives for removal.\nThe following code segment details the methodology applied:\n\n\nCode\n# List of years\nyears &lt;- 1995:2014\n\n# Read all datasets and combine\nall_surveys_description &lt;- map_df(years, function(year) {\n  df &lt;- read_dta(paste0(ros_data_loc, year, \"/res_deb.dta\"))\n    # Convert all columns to character to ensure consistency\n  df &lt;- df %&gt;% mutate_all(as.character)\n  return(df)\n})\n\n# Extract unique combinations and list all the years they appeared in\nunique_combinations &lt;- all_surveys_description %&gt;%\n  group_by(j0, j42, j4) %&gt;%\n  summarize(years = toString(unique(year)),\n            obs_count = n()) %&gt;%\n  ungroup()\n\n# Harmonize the fields that contain municipality or village names\nunique_combinations &lt;- unique_combinations %&gt;%\n  mutate(clean_muni = clean_string(j42),\n         clean_village = clean_string(j4))\nobs_communes &lt;- obs_communes %&gt;%\n  mutate(clean_ADM3 = clean_string(ADM3_EN))\npop_places &lt;- pop_places %&gt;%\n  mutate(clean_pname = clean_string(PLACE_NAME)) \n\n# List of observatories for which municipalities have been identified\nidentified_observatories &lt;- unique(obs_communes$OBS_CODE) %&gt;%\n  na.omit()\n# Filter for the observatory for which we already have a manual identification\n# of municipalities\nunique_combinations &lt;- unique_combinations %&gt;%\n  filter(j0 %in% identified_observatories) \n\n# Apply the fuzzy matching observatory-wise\nresults &lt;- map2_df(unique_combinations$clean_muni, \n                   unique_combinations$j0, \n                   ~ as.data.frame(t(\n                     fuzzy_match(.x, obs_communes, \"clean_ADM3\", .y))))  %&gt;%\n  unnest(cols = c(matched_string, distance, ADM3_PCODE, ADM3_EN))\n\n# Combine the results with the unique_combinations\ncorrespondence_table &lt;- bind_cols(unique_combinations, results) \n\n# Add a column for the matching method\ncorrespondence_table &lt;- correspondence_table %&gt;%\n  mutate(method = ifelse(!is.na(matched_string), \"method_1\", NA_character_))\n\n\nTo synthesize and visually represent our matches, the identified municipalities are plotted on a map:\n\n\nCode\n# Extract matched resuts\nmatched_results &lt;- unique(correspondence_table$ADM3_PCODE) %&gt;%\n  na.omit()\n# Keep municipalities in those\nmatched_spatial &lt;- obs_communes %&gt;%\n  filter(ADM3_PCODE %in% matched_results)\n\ntmap_mode(\"view\")\n# Plot\ntm_shape(matched_spatial) +\n  tm_polygons(col = \"OBS_NAME\")\n\n\n\n\n\n\n\n\n\n3.3.2 Method 2: Extracting municipality names from text in “village names”\nFor observations where the field “municipality name” was empty (all cases before 2004 and still a frequent situation afterwards), we turned our attention to the village name field. The objective was to determine whether these village names could potentially contain a municipality’s name that had been previously identified during the observatory surveys. For the observations with no municipality value, we extracted the first word or segment in the village name. This extracted word then underwent a process of fuzzy matching against the list of identified municipalities’ names. However, manual verification identified errors. Certain matches, which we labeled as ‘false positives’, were found to be errors and were removed from the results. Once this cleansing step was completed, we included the validated matches back into the primary correspondence table.\n\n\nCode\n# Filter out matched municipalities and extract the first word\nunmatched_results &lt;- correspondence_table %&gt;%\n  filter(is.na(matched_string)) %&gt;%\n  select(j0:clean_village) %&gt;%\n  mutate(first_word = str_extract(clean_village, \"^[^\\\\s/]+\"))\n\n# Fuzzy matching with the first word and identified municipalities\nresults_step2 &lt;- map2_df(unmatched_results$first_word, \n                         unmatched_results$j0, \n                         ~ as.data.frame(t(\n                           fuzzy_match(.x, obs_communes, \"clean_ADM3\", .y))))  %&gt;%\n  unnest(cols = c(matched_string, distance, ADM3_PCODE, ADM3_EN))\n\n# Update Results\npotential_matches2 &lt;- bind_cols(unmatched_results, results_step2)\n\n# Manually identify false positives and remove them\nfalse_positives &lt;- c(\"madiromionga\", \"maroarla\", \"tsaratanteraka\", \n                    \"ambatoharanana\", \"ambatoaranana\", \"maroala\", \"erakoja\", \n                    \"erakoka\", \"maroalo\", \"erakka\", \"erakoa\")\nvalidated_matches2 &lt;- potential_matches2 %&gt;%\n  mutate(across(c(matched_string, ADM3_PCODE, ADM3_EN, distance),\n               ~ ifelse(first_word %in% false_positives, NA, .)),\n         method = ifelse(!is.na(matched_string), \"method_2\", NA_character_))\n\n# Integrate new results in correspondence table\ncorrespondence_table &lt;- correspondence_table %&gt;%\n  filter(!is.na(matched_string)) %&gt;%\n  bind_rows(validated_matches2) %&gt;%\n  select(-first_word)\n\n\n\n\n3.3.3 Method 3: Village name fuzzy matching with populated places\nWith some observations still devoid of a matched municipality, we initiated another processing layer. This phase saw the unmatched village names being subjected to fuzzy matching against the populated places dataset described above. We first sifted out the unmatched results from our prior analysis. Then, we augmented the ‘pop_places’ dataset with observatory codes for the entries that were located in communes recognized as surveyed by observatories. Next we performed another fuzzy matching, comparing the village names against the ‘pop_places’ names. To maintain a level of precision, we set a restrictive threshold, dismissing any match that exceeded a certain “distance” or degree of difference. But, as with the previous methods, visual verification revealed some mismatches. These ‘false positives’ were flagged and discarded and the remaining ones were integrated into the overarching correspondence table.\n\n\nCode\n# Re-filter unmatched results\nunmatched_results2 &lt;- correspondence_table %&gt;%\n  filter(is.na(matched_string)) %&gt;%\n  select(j0:clean_village)\n\n# Join OBS_CODE to pop_places\npop_places &lt;- pop_places %&gt;%\n  rename(ADM3_PCODE = COM_PCODE, ADM3_EN = COMMUNE) %&gt;%\n  mutate(ADM3_PCODE = str_replace(ADM3_PCODE, \"^MDG\", \"MG\")) %&gt;%\n  left_join(select(obs_communes, ADM3_PCODE, OBS_CODE) %&gt;%\n                     st_drop_geometry(), \n            by = \"ADM3_PCODE\")\n\n# Apply the fuzzy matching observatory-wise\nresults_step3 &lt;- map2_df(unmatched_results2$clean_village, \n                         unmatched_results2$j0, \n                         ~ as.data.frame(t(\n                           fuzzy_match(.x, pop_places, \"clean_pname\", .y,\n                                       max_distance = 0.22))))  %&gt;% # erratic results beyond\n  unnest(cols = c(matched_string, distance, ADM3_PCODE, ADM3_EN))\n\n# Bind the results with unmatched_results_v2\npotential_matches3 &lt;- bind_cols(unmatched_results2, results_step3)\n\n# Manually identify false positives and remove them\nfalse_positives2 &lt;- c(\"analambarika\", \"maroaloka\", \"ambakela\", \"ambodirofia\",\n                      \"ambohibao\", \"ambato mangabe\", \"marofonaritra\", \n                      \"andranovo ambodimanga\", \"morataitra\", \"antanambao\", \n                      \"ankililoa\")\nvalidated_matches3 &lt;- potential_matches3 %&gt;%\n  mutate(across(c(matched_string, ADM3_PCODE, ADM3_EN, distance),\n               ~ ifelse(clean_village %in% false_positives2, NA, .)),\n         method = ifelse(!is.na(matched_string), \"method_3\", NA_character_))\n\ncorrespondence_table &lt;- correspondence_table %&gt;%\n  filter(!is.na(matched_string)) %&gt;%\n  bind_rows(validated_matches3)\n\n\n\n\n3.3.4 Method 4\nFor the remaining, we try matching with other village names for which the municipality has been matched.\nEven after the above measures, there remained observations that had eluded a municipality match. The subsequent strategy was to juxtapose them with other village names that had already been successfully matched to a municipality. We assembled a table comprising municipality names and their paired village names, using data from previous successful matches. Then the still-unmatched village names were run through a fuzzy matching process against the known village names. We fine-tuned the algorithm with a restrictive matching threshold. Again, some matches stood out as anomalies. Labeled as ‘false positives’, these were sifted out and the newly matched data was incorporated into the main correspondence table.\n\n\nCode\n# Create a list of municipality names and village names for matched observations\nmatched_villages &lt;- correspondence_table %&gt;%\n  filter(!is.na(method)) %&gt;%\n  select(j0, ADM3_PCODE, ADM3_EN, clean_village) %&gt;%\n  distinct() %&gt;%\n  rename(OBS_CODE = j0)\n\n# Re-filter unmatched results\nunmatched_results3 &lt;- correspondence_table %&gt;%\n  filter(is.na(method)) %&gt;%\n  select(j0:clean_village)\n\n# Try matching unmatched villages against matched village names observatory-wise\nresults_village_match &lt;- map2_df(\n  unmatched_results3$clean_village, unmatched_results3$j0,\n  ~ as.data.frame(t(fuzzy_match(.x, matched_villages, \"clean_village\",\n                                .y, max_distance = 0.28)))) %&gt;%\n  unnest(cols = c(matched_string, distance, ADM3_PCODE, ADM3_EN))\n# Bind these results with unmatched_results_v3\npotential_matches4 &lt;- bind_cols(unmatched_results3, results_village_match)\n\n# Manually identify false positives and remove them\nfalse_positives4 &lt;- c(\"amp0mbibitika antanakova\", \"arakoke ambonano ampihamibe\", \n                      \"farara farara\", \"farara ambakela\", \"fara ambakela\", \n                      \"ambatotelo marofinaritra\")\nvalidated_matches4 &lt;- potential_matches4 %&gt;%\n  mutate(across(c(matched_string, ADM3_PCODE, ADM3_EN, distance),\n               ~ ifelse(clean_village %in% false_positives4, NA, .)),\n         method = ifelse(!is.na(matched_string), \"method_4\", NA_character_))\n\n\n# Merge the updated results back to the correspondence table\ncorrespondence_table &lt;- correspondence_table %&gt;%\n  filter(!is.na(matched_string)) %&gt;%\n  bind_rows(validated_matches4)"
  },
  {
    "objectID": "03-ros-data-georeferencing.html#matching-villages",
    "href": "03-ros-data-georeferencing.html#matching-villages",
    "title": "3  Georeferencing",
    "section": "3.4 Matching villages",
    "text": "3.4 Matching villages\nAfter successfully matching commune names with an official, normalized reference, our next goal is to align the villages as identified by the ROS with their administrative counterpart, known as Fokontany. This task presents challenges, notably because the ROS village identifications weren’t directly linked to Fokontany names, and the Fokontany names themselves have undergone changes. Additionally, variations in village naming within the ROS dataset complicate the process. Our approach involves several steps: First, we load the official COD Fokontany dataset. Next, we standardize the place names using our previously discussed clean_string function. Then, using the purpose-built fuzzy_match_village function, we assess the similarity between the village names in the ROS dataset and the Fokontany names within the same commune, specifically by calculating the Jaro-Winkler distance between the strings(see Section 4.2.1 for more details on the Jaro-Winkler distance). We select the names with the highest degree of matching. During a manual review of these results, we closely examined the matches, especially those with larger Jaro-Winkler distance. From this examination, we derived a distance of 0.2, below which the majority of matches were accurate. Notably, some matches above this threshold were deemed accurate and retained, while a small number of matches below this threshold were identified as false positives and subsequently discarded.\n\n\nCode\n# Load the ADM4 dataset, which corresponds to Fokontany\nfokontany &lt;- st_read(\"data/Spatial_data/OCHA_BNGRC admin boundaries/mdg_admbnda_adm4_BNGRC_OCHA_20181031.shp\",\n                      quiet = TRUE) %&gt;%\n  mutate(ADM4_clean = clean_string(ADM4_EN))\n\n# A function to perform fuzzy matching with Fokontany or populated places\nfuzzy_match_village &lt;- function(target_string, dataframe, column_name, \n                                ADM3_PCODE_filter, id_column, \n                                municipality_string = NULL, max_distance = 0.4) {\n  \n  # Check if target_string has multiple words and if the first word matches the municipality_string\n  if (!is.null(municipality_string) && length(unlist(strsplit(target_string, \" \"))) &gt; 1) {\n    first_word &lt;- unlist(strsplit(target_string, \" \"))[1]\n    distances_municipality &lt;- stringdist::stringdist(first_word, municipality_string, method = \"jw\")\n    if (min(distances_municipality, na.rm = TRUE) &lt;= max_distance) {\n      target_string &lt;- str_remove(target_string, paste0(\"^\", first_word, \" \"))\n    }\n  }\n  \n  filtered_reference &lt;- dataframe %&gt;%\n                          filter(ADM3_PCODE == ADM3_PCODE_filter) %&gt;%\n                          select(all_of(column_name), all_of(id_column))\n  \n  # If filtered_reference is empty, return NA values\n  if (nrow(filtered_reference) == 0) {\n    return(list(matched_string = NA, ID = NA, geometry = NA, distance = 1))\n  }\n  \n  # Use stringdist to find the closest match\n  distances &lt;- stringdist::stringdistmatrix(\n    target_string, filtered_reference[[column_name]], method = \"jw\")\n  \n  # If there are no valid distances, set min_distance to Inf\n  if(all(is.na(distances))) {\n    min_distance &lt;- Inf\n  } else {\n    min_distance &lt;- min(distances, na.rm = TRUE)\n  }\n  \n  # Check for Inf distance and replace it with 1\n  if (is.infinite(min_distance)) {\n    min_distance &lt;- 1\n  }\n  \n  # If min_distance exceeds the max_distance threshold, return NA values\n  if (min_distance &gt; max_distance) {\n    return(list(matched_string = NA, ID = NA, geometry = NA, \n                distance = NA))\n  }\n  \n  matched_row &lt;- filtered_reference[which.min(distances), ]\n  \n  return(list(matched_string = matched_row[[column_name]], \n              ID = matched_row[[id_column]], \n              distance = min_distance))\n}\n# correspondence table without municipality geometry\ncorrespondence_table2 &lt;- correspondence_table %&gt;%\n  st_drop_geometry()\n\n# For fokontany\nresults_fokontany &lt;- pmap_df(\n  list(clean_village = correspondence_table2$clean_village, \n       ADM3_PCODE = correspondence_table2$ADM3_PCODE, \n       municipality_string = correspondence_table2$clean_muni),\n  function(clean_village, ADM3_PCODE, municipality_string) {\n    as.data.frame(t(fuzzy_match_village(clean_village, fokontany, \n                                        \"ADM4_clean\", ADM3_PCODE, \"ADM4_PCODE\", municipality_string)))\n  }\n) %&gt;%\n   unnest(cols = everything()) %&gt;%\n  rename(ADM4_PCODE = ID, ADM4_clean = matched_string, distance2 = distance) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(-geometry)\n \n# Combine the results with correspondence_table\ncorrespondence_table2_updated &lt;- bind_cols(correspondence_table2, \n                                          results_fokontany) %&gt;%\n  mutate(method_village = ifelse(!is.na(ADM4_PCODE), \"method_5\", NA_character_))\n\n\n# For a visual validation\ncorrespondence_table2_updated %&gt;% \n  arrange(desc(distance2)) %&gt;%\n  write_xlsx(\"correspondence_table2_update.xlsx\")\n\n# After instpecting the data, we find that a 0.2 threshold in distance is \n# appropriate. However some valid matches above this threshold should be kept\n\nhigh_tolerance_valid_matches &lt;- c(\n  \"ambanja\", \"ambararata 2\", \"ambaro\", \"ambatoharanana\", \"ambatomanga\", \n  \"ambazoamazava\", \"ambazoamirafy\", \"ambodibonara\", \"ambodimotso atsimo\", \n  \"ambohidrony\", \"ambohimahatsinjo atsinanana\", \"ambongabe\", \"amparafaravola\", \n  \"ampasy\", \"ampijoroa\", \"ampijoroan ala\", \"ampitana\", \"analambarika\", \n  \"andohasoamahainty\", \"anjiamangirana 1\", \"ankazoabo anivo\", \"ankerereake\", \n  \"ankililoaka 2\", \"bepako\", \"bevato\", \"erada 2\", \"esanta fototra\", \n  \"esanta marofoty\", \"esanta maromainty\", \"feramanga atsimo\", \"habohabo atsimo\", \n  \"habohabo avaratra\", \"lafiatsinanana\", \"madiromiongana\", \"manambaro 2\", \n  \"manantenina\", \"mangabe\", \"mangaoka\", \"maritampona\", \"maroala\", \"marofoty\", \n  \"marolampy\", \"marovantaza\", \"miary\", \"miary ankasy\", \"miary ankoronga\", \n  \"miorimivalana\", \"tanambao\", \"tanambao 2\", \"tandroka andrefana\", \n  \"tsarahasina\", \"vohilengo\")\n\n# Below this threshold, a few false positives should be removed\nlow_tolerance_invalid_marches &lt;- c(\n  \"AMBOHIMAHATSINJO EST\", \"AMBOHIMAHATSINJO-EST/CENTRE\", \n  \"AMBALAKINDRESY/ANDOHARENINA\", \"ambazoa /ampiha\", \"AMBALAKINDRESY/ANDOHARENA\", \n  \"ANKAZOABO SUD/ALAMBARIKE\", \"ANKARINEZAKA/NAMARINA\", \"AMBALA/ANDRANOLAVA\", \n  \"AMBALAKINDRESY/ANDRANOLAVA\", \"AMBALAKINDRESY/ANDRANOLAVA\")\n\ncorrespondence_table3 &lt;- correspondence_table2_updated %&gt;%\n  mutate(\n    across(c(ADM4_clean, ADM4_PCODE, distance2, method_village), \n           ~ case_when(\n             (distance2 &gt; 0.2 & !ADM4_clean %in% high_tolerance_valid_matches) |\n             ADM4_clean %in% low_tolerance_invalid_marches ~ NA,\n             TRUE ~ .)))"
  },
  {
    "objectID": "03-ros-data-georeferencing.html#validating-the-quality-of-georeferenced-data",
    "href": "03-ros-data-georeferencing.html#validating-the-quality-of-georeferenced-data",
    "title": "3  Georeferencing",
    "section": "3.5 Validating the Quality of Georeferenced Data",
    "text": "3.5 Validating the Quality of Georeferenced Data\nTo ensure the robustness and validity of our georeferenced data, we employ a multi-tiered validation approach that hinges on quantitative metrics and qualitative consistency checks.\n\n3.5.1 Quantitative metrics\nWe first look at the distribution of the unique observations that were successfully matched during the various stages of the georeferencing process.\n\n\nCode\n# Re-format modality labels  \nmatching_tbl &lt;- correspondence_table3 %&gt;%\n  mutate(method = case_when(\n    method == \"method_1\" ~ \"Method 1\",\n    method == \"method_2\" ~ \"Method 2\",\n    method == \"method_3\" ~ \"Method 3\",\n    method == \"method_4\" ~ \"Method 4\",\n    is.na(method) ~ \"Unmatched communes\"),\n    method_village = case_when(\n      method_village == \"method_5\" ~ \"Matched villages\",\n      is.na(method_village) ~ \"Unmatched villages\"))\n\nsummarized_data &lt;- matching_tbl %&gt;%\n  group_by(method, method_village) %&gt;%\n  summarize(total_obs = sum(obs_count, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(percent_obs = total_obs/sum(total_obs))\n\ncount_tbl &lt;- summarized_data %&gt;%\n  select(method, method_village, total_obs) %&gt;%\n  pivot_wider(names_from = method_village, values_from = total_obs, \n              values_fill = 0) %&gt;%\n  rowwise() %&gt;%\n  mutate(Total = sum(c(`Matched villages`, `Unmatched villages`))) %&gt;%\n  ungroup()  %&gt;%\n  adorn_totals(\"row\")\n\npercent_tbl &lt;- summarized_data %&gt;%\n  select(method, method_village, percent_obs) %&gt;%\n  pivot_wider(names_from = method_village, values_from = percent_obs, \n              values_fill = 0) %&gt;%\n  rowwise() %&gt;%\n  mutate(`Total (%)` = sum(c(`Matched villages`, `Unmatched villages`))) %&gt;%\n  ungroup() %&gt;%\n  rename(`Matched villages (%)` = `Matched villages`,\n         `Unmatched villages (%)` = `Unmatched villages`) %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x * 100, 2)),\n         across(where(is.numeric))) %&gt;%\n  adorn_totals(\"row\")\n\n\nsummarized_data2 &lt;- count_tbl %&gt;%\n  left_join(percent_tbl, by = \"method\") %&gt;%\n  relocate(method, `Matched villages`, `Matched villages (%)`, \n           `Unmatched villages`, `Unmatched villages (%)`,\n           Total, `Total (%)`) %&gt;%\n  rename(`Commune matching method` = method)\n\ngt(summarized_data2)  %&gt;%\n  # Add a title\n  tab_header(title = \"Result of matching process\",\n             subtitle = \"Number of survey observation per method\") \n\n\n\n\n\n\nTable 3.1:  Number of ROS observation which commune and village were matched \n  \n    \n      Result of matching process\n    \n    \n      Number of survey observation per method\n    \n    \n      Commune matching method\n      Matched villages\n      Matched villages (%)\n      Unmatched villages\n      Unmatched villages (%)\n      Total\n      Total (%)\n    \n  \n  \n    Method 1\n30925\n30.69\n12605\n12.51\n43530\n43.20\n    Method 2\n16491\n16.37\n6727\n6.68\n23218\n23.04\n    Method 3\n10723\n10.64\n4006\n3.98\n14729\n14.62\n    Method 4\n3843\n3.81\n7198\n7.14\n11041\n10.96\n    Unmatched communes\n0\n0.00\n8237\n8.18\n8237\n8.18\n    Total\n61982\n61.51\n38773\n38.49\n100755\n100.00\n  \n  \n  \n\n\n\n\n\n\n\n3.5.2 Consistency check with documentation\nTo ensure the comprehensiveness of our georeferencing process, we summarize the list of unique matches for villages from all the municipalities in the final dataset. This check will help ensure there are no glaring gaps in the matched data.\n\n\nCode\nfokontany_list &lt;- correspondence_table3 %&gt;%\n  mutate(j0 = as.numeric(j0)) %&gt;%\n  left_join(observatory_names, by = c(\"j0\" = \"code\")) %&gt;%\n  rename(OBS_NAME = name) %&gt;%\n  left_join(select(st_drop_geometry(fokontany), ADM4_PCODE, ADM4_EN),\n            by = \"ADM4_PCODE\") %&gt;%\n  group_by(OBS_NAME, ADM3_EN) %&gt;%\n  summarize(ADM4_EN = list(unique(if_else(is.na(ADM4_EN), \"Unknown\", ADM4_EN)))) %&gt;%\n  ungroup() %&gt;%\n  mutate(ADM4_EN = map_chr(ADM4_EN, ~ str_c(.x, collapse = \", \")))\n\nfokontany_list %&gt;%\n  rename(`Observatory Name` = OBS_NAME,\n         `Commune Name` = ADM3_EN,\n         `Fokontany Names` = ADM4_EN) %&gt;%\n  DT::datatable(caption = \"List of fokontany per commune and observatory\")\n\n\n\n?(caption)\n\n\n\n\n\n\n\n\n\n\nWe compare this with the ROS documentation to assess which villages might have been omitted by our algorithm.\n\n\n3.5.3 Visual exploration and avenues for continuous corrections\nWe visually explore the the georeferenced data to observe its spatial distribution and spot eventual anomalies or clustering patterns that might not be immediately obvious from numerical summaries. The visualization below maps out our matched municipalities alongside villages. Discrepancies or clusters may indicate a need for further refinement or correction.\n\n\nCode\ncommmune_pcode_years &lt;- correspondence_table3 %&gt;%\n  # Split and unnest the years\n  mutate(years = str_split(years, \", \")) %&gt;%\n  unnest(years) %&gt;%\n  # Convert years to numeric for proper sorting\n  mutate(years = as.numeric(years)) %&gt;%\n  # Group by ADM3_PCODE and extract unique, sorted years\n  group_by(ADM3_PCODE) %&gt;%\n  summarize(years = list(unique(years))) %&gt;%\n  ungroup() %&gt;%\n  # Sort and collapse the years\n  mutate(years = map_chr(years, ~ paste(sort(.x), collapse = \", \")))\n\nfokontany_pcode_years &lt;- correspondence_table3 %&gt;%\n  # Split and unnest the years\n  mutate(years = str_split(years, \", \")) %&gt;%\n  unnest(years) %&gt;%\n  # Convert years to numeric for proper sorting\n  mutate(years = as.numeric(years)) %&gt;%\n  # Group by ADM3_PCODE and extract unique, sorted years\n  group_by(ADM4_PCODE) %&gt;%\n  summarize(years = list(unique(years))) %&gt;%\n  ungroup() %&gt;%\n  # Sort and collapse the years\n  mutate(years = map_chr(years, ~ paste(sort(.x), collapse = \", \")))\n\nselected_communes &lt;- commmune_pcode_years %&gt;%\n  filter(!is.na(ADM3_PCODE)) %&gt;%\n  left_join(obs_communes, by = \"ADM3_PCODE\") %&gt;%\n  st_sf()\n\nselected_fokontany &lt;- fokontany_pcode_years %&gt;%\n  filter(!is.na(ADM4_PCODE)) %&gt;%\n  left_join(fokontany, by = \"ADM4_PCODE\") %&gt;%\n  st_sf()  %&gt;%\n  mutate(label = \"Surveyed______\")\n\nwrite_rds(selected_communes, \"output/selected_communes.rds\")\nwrite_rds(selected_fokontany, \"output/selected_fokontany.rds\")\n\ntmap_mode(\"view\")\ntm_shape(selected_communes) + \n  tm_fill(col = \"OBS_NAME\", palette = \"Set1\", title = \"Observatory\",\n          id = \"ADM3_EN\", legend.show = FALSE,\n          popup.vars = c(\"Observatory\" = \"OBS_NAME\",\n                         \"Data collection years\"  = \"years\",\n                         \"District\" = \"ADM2_EN\",\n                         \"Region\" = \"ADM1_EN\")) +\n  tm_shape(selected_fokontany) + \n  tm_fill(col = \"label\", palette = c(\"black\"), alpha = 0.6, title = \"Fokontany\", \n          id = \"ADM4_EN\", legend.show = FALSE,\n          popup.vars = c(\"Observatory\" = \"OBS_NAME\",\n                         \"Data collection years\"  = \"years\",\n                         \"Commune\" = \"ADM3_EN\",\n                         \"District\" = \"ADM2_EN\",\n                         \"Region\" = \"ADM1_EN\")) +\n  tm_scale_bar()\n\n\n\n\n\n\nFigure 3.2: Map of the surveyed communes and fokonany\n\n\n\nFrom this summary table, we see that the communes could not be re-identified for 8.18% of the observations and the village could not be re-identified for 38.49% of observations. A manual work to identify villages will have to be implemented to be able to improve this matching."
  },
  {
    "objectID": "03-ros-data-georeferencing.html#conclusion",
    "href": "03-ros-data-georeferencing.html#conclusion",
    "title": "3  Georeferencing",
    "section": "3.6 Conclusion",
    "text": "3.6 Conclusion\nGiven the variability in location labels recorded in the ROS data, we emphasized standardizing and aligning toponyms. We introduced the clean_string function to unify place names, addressing variances in language, letter casing, and other descriptors. Utilizing fuzzy matching, we connected similar text strings, using the Levenshtein distance as a benchmark. To heighten accuracy, a hierarchical data matching strategy was applied, reflecting the inherent structure of geographical units, where regions envelop districts, which further consist of municipalities and localities. The COD data, curated by BNGRC and OCHA, was our reference point for these matches. Various techniques, including hierarchical fuzzy matching and visual checks, were employed.\nAs a result, we could automatically match 98% of commune name variations in the dataset with an official commune, and 62% of village name variations with a fokontany. The outcomes are consistent with ROS’s internal documentation, and the spatial representations can be further inspected for validation. Future corrections and enhancements are anticipated, and our system, leveraging Quarto notebook and reproducible R code, will facilitate these refinements.\nROS began its data collection at a time when geolocation best practices weren’t as evolved as they are today. Should more survey rounds be conducted in the future, the intricate georeferencing process demonstrated here should serve both as a cautionary tale and a guide on optimizing data registration (for instance, using PCODES). We’re optimistic that refining the georeferencing of past ROS data, as presented in this work, will amplify the data’s utility for both research and policy decisions."
  },
  {
    "objectID": "04-ros-data-preparation.html#data-selection",
    "href": "04-ros-data-preparation.html#data-selection",
    "title": "4  Data preparation",
    "section": "4.1 Data selection",
    "text": "4.1 Data selection\nFirst, we start with creating a copy of the original unfiltered and un-anonymized data.\n\n\nCode\n# Define the paths for the source and target folders\nsource_folder &lt;- \"data/ROS_data_original\"\ntarget_folder &lt;- \"data/dta_format\"\n\n# Create the target folder if it does not exist\nif (!dir.exists(target_folder)) {\n  dir.create(target_folder, recursive = TRUE)\n}\n\n# Empty the target folder if it already contains files\nif (length(list.files(target_folder, recursive = TRUE)) &gt; 0) {\n  # List all files within the target folder recursively\n  files_to_remove &lt;- list.files(target_folder, full.names = TRUE, recursive = TRUE)\n  # Remove these files\n  file.remove(files_to_remove)\n}\n\n# Function to recursively copy files from source to target\ncopy_files &lt;- function(source, target) {\n  # Ensure the target directory exists\n  if (!dir.exists(target)) {\n    dir.create(target, recursive = TRUE)\n  }\n  \n  # List all files and directories in the source\n  contents &lt;- list.files(source, full.names = TRUE)\n  \n  # Separate files and directories\n  dirs &lt;- contents[which(sapply(contents, function(x) file.info(x)$isdir))]\n  files &lt;- contents[which(sapply(contents, function(x) !file.info(x)$isdir))]\n  \n  # Copy files\n  if (length(files) &gt; 0) {\n    file.copy(files, target, overwrite = TRUE)\n  }\n  \n  # Recursively copy directories\n  if (length(dirs) &gt; 0) {\n    for (dir in dirs) {\n      new_source &lt;- dir\n      new_target &lt;- file.path(target, basename(dir))\n      copy_files(new_source, new_target)\n    }\n  }\n}\n\n# Copy all files and folders from source to target\ncopy_files(source_folder, target_folder)\n\n\nFor 2015, we have 4 observatories. One that existed in the previous years, Menabe Nord-Est, and 3 new ones: Ambatofinandrahana, Anjozorobe et Maintirano. The data collection of the ROS continued until 2017, but we lack documentation since 2015 and the data has not yet been harmonized for 2016 and 2017. For this reason, we only kept the data for Menabe North-East for 2015.\n\n\nCode\n# Define the path to the 2015 folder within the target folder\nfolder_2015 &lt;- \"data/dta_format/2015\"\n\n# List all .dta files in the 2015 folder\ndta_files &lt;- list.files(folder_2015, pattern = \"\\\\.dta$\", full.names = TRUE)\n\n# Initialize a variable to track if all files were successfully filtered\nall_files_filtered &lt;- TRUE\n\n# Get households in selected observatory (Menabe Nord-Est)\nhh_to_keep &lt;- read_dta(\"data/dta_format/2015/res_deb.dta\") %&gt;%\n  filter(j0 == 52) %&gt;%\n  pluck(\"j5\")\n\n# Loop through each .dta file\nfor (file_path in dta_files) {\n  # Load the dataset\n  data &lt;- read_dta(file_path)\n  # Check if 'j5' exists and is a character variable\n  if (\"j5\" %in% names(data) && is.character(data$j5)) {\n    # Filter for j5 \n    filtered_data &lt;- data %&gt;%\n      filter(j5 %in% hh_to_keep)\n    # Save the filtered dataset back to the same file (or to a new file/path)\n    write_dta(filtered_data, file_path)\n  } else {\n    # Set the flag to FALSE if j5 does not exist or is not character in any file\n    all_files_filtered &lt;- FALSE\n    break  # Exit the loop as we found a file not meeting the criteria\n  }\n}\n\n# Check if all files were successfully filtered and print a message\nif (!all_files_filtered) {\n  cat(\"Error: Not all files were filtered. At least one file does not contain 'j5' as a character variable.\")\n}\n\n\nIf no error message is displayed, the filtering went correctly."
  },
  {
    "objectID": "04-ros-data-preparation.html#data-anonymization",
    "href": "04-ros-data-preparation.html#data-anonymization",
    "title": "4  Data preparation",
    "section": "4.2 Data anonymization",
    "text": "4.2 Data anonymization\n\n4.2.1 Anonymization of survey respondents\nDuring the rural observatory surveys, the names of the household members were collected in the questionnaire called the roaster. To prevent re-identification of personal data, we will replace these names with pseudonyms such as “individual_01”, “individual_02”, and so on. These pseudonyms are not related to the original names and individuals with the same name in different households will be given different pseudonyms. However, the same household members will have the same pseudonym in subsequent surveys. For example, in a particular household, “individual_05” in 1998 is the same person as “individual_05” in the 1999, 2000, 2001, and 2002 survey rounds.\nThe main challenge with this procedure is that the names were provided orally by the respondent, written down by the surveyors, and later entered into the system by data entry clerks. As a result, we have a wide range of variations in the character strings in our data, even though they correspond to the names of the same individuals. To carry out this pseudonymization process, we follow several steps that involve fuzzy matching and consistency checks with individual age and sex. We begin by loading and consolidating the content of the survey rosters for all survey years.\n\n\nCode\n# Usage\nros_data_loc &lt;- \"data/dta_format/\"\nyears &lt;- 1995:2015\n\n# Normalizing function \nnormalize_name &lt;- function(name) {\n  name %&gt;%\n    stringi::stri_trans_general(\"Latin-ASCII\") %&gt;%\n    str_to_lower() %&gt;%\n    str_replace_all(\"[^a-z ]\", \"\") %&gt;%\n    str_trim()\n}\n\n# Function to read and preprocess data\nread_and_normalize &lt;- function(year, ros_data_loc) {\n  \n  file_name &lt;- if (year == 1995) \"res_m.dta\" else \"res_m_a.dta\"\n  file_path &lt;- file.path(ros_data_loc, as.character(year), file_name)\n  \n  if (!file.exists(file_path)) return(NULL)\n  \n  read_dta(file_path) %&gt;%\n    select(m1, year, j5, m4, m5) %&gt;%\n    mutate(m5 = as.numeric(m5),\n           name_normalized = normalize_name(m1),\n           line_number = row_number())\n}\n\nall_data &lt;- map_df(years, ~read_and_normalize(.x, ros_data_loc))\n\n# Get a list of unique household IDs\nhousehold_ids &lt;- unique(all_data$j5)\n\n# Count household ids and observations\nnb_hh &lt;- nrow(household_ids)\nnb_i &lt;- nrow(all_data)\n\n\nWe have a total of 590,524 individual observations of 29,493 unique households. To match name variations within subsequent surveys of the same household, we use the Jaro-Winkler algorithm as implemented in the stringdist package. This algorithm is described as follows by the package author [van_der_loo_stringdist_2014, p. 119]:\n\n“The Jaro distance was originally developed at the U.S. Bureau of the Census for the purpose of linking records based on inaccurate text ﬁelds. (…) It has been successfully applied to statistical matching problems concerning fairly short strings, typically name and address data (Jaro 1989). The reasoning behind the Jaro distance is that character mismatches and transpositions are caused by typing or phonetic transcription errors but matches between remote characters are unlikely to be caused by such kind of errors. (…) Winkler (1990) extended the Jaro distance by incorporating an extra penalty for character mismatches in the first four characters. (…) The reasoning is that apparently, people are less apt to make mistakes in the first four characters or perhaps they are more easily noted, so differences in the first four characters point to a larger probability of two strings being actually different.”\n\nWe refined a procedure that applies this algorithm in three steps:\n\nInitial reference: The initial survey year of each household, member names are cataloged to serve as a reference;\nClose Match Identification: For each ensuing survey, we scout for names that not only exhibit the smallest Jaro-Winkler distance from the reference names but also fall below a stringent threshold of 0.2 (ie. we only take into account the name when the names are very similar);\nExpanded Criteria for Matches: Absence of matches at step 2. for a given year prompts an extended search within the household, this time accommodating names with a distance below 0.3 if they align in sex and age, accounting for a 5-year margin to mitigate inaccuracies in age recall (i.e. we allow for slightly more dissimilar names if sex and age match);\nValidation of Matches: For each match identified at step 2. or 3., we verify that there is no other household member name that is a better match based on the Jaro-Winkler distance. If so, we remove it from the matched names.\nPseudonym Assignment: Matched names get a pseudonym “Individual_XX”, with “XX” representing a sequential number..\nSequential Application: This procedure iterates through all names from the initial survey year, extending to unmatched names in subsequent years, thereby ensuring comprehensive coverage.\n\nThe code was adapted to handle gracefully edge cases, for instance when sex data or age is missing.\n\n\nCode\npseudonymize_household &lt;- function(pool, distance_threshold1 = 0.2, \n                                   distance_threshold2 = 0.3,\n                                   tolerance_yeardiff = 5) {\n  years &lt;- unique(pool$year) # extract list of existing years in dataset\n  pseudonymized &lt;- data.frame() # create empty dataframe\n  next_pseudonym_id &lt;- 1 # initialize the pseudonym id\n  \n  for (current_year in years) {\n    staging &lt;- subset(pool, year == current_year)\n    # For subsequent years, attempt to match with existing pseudonyms\n    for (i in seq_len(nrow(staging))) {\n      name &lt;- staging$name_normalized[i]\n      sex &lt;- staging$m4[i]\n      age &lt;- staging$m5[i]\n      \n      pool &lt;- pool %&gt;%\n        mutate(dist = stringdist(name_normalized, name, method = \"jw\"),\n               age_diff = abs(m5 - age - (year - current_year))) %&gt;%\n        group_by(year) %&gt;%\n        mutate(\n          match = case_when(\n            # First level of matching based on distance_threshold1\n            dist == min(dist) & dist &lt; distance_threshold1 ~ \"matched\",\n            \n            # Second level of matching based on distance_threshold2\n            dist == min(dist) & dist &lt; distance_threshold2 & \n              (is.na(m4) | m4 == sex) & age_diff &lt;= tolerance_yeardiff ~ \"matched\",\n            TRUE ~ \"unmatched\"), # default\n          pseudonym = ifelse(match == \"matched\", \n                             sprintf(\"individual_%02d\", next_pseudonym_id), \n                             NA_character_)) %&gt;%\n        ungroup()\n      \n      # Ensure 'match' column is explicitly treated as a character\n      pool$match &lt;- as.character(pool$match)\n      \n      # Then perform the operation to compute min_dist_unmatch and re-evaluate 'match'\n      if (any(pool$match == \"matched\")) {\n        unmatched_names &lt;- pool$name_normalized[pool$match == \"unmatched\"]\n        pool &lt;- pool %&gt;%\n          rowwise() %&gt;%\n          mutate(min_dist_unmatch = if_else(match == \"matched\" & \n                                              length(unmatched_names) &gt; 0,\n                                            min(stringdist(name_normalized, \n                                                           unmatched_names, \n                                                           method = \"jw\"), \n                                                na.rm = TRUE),\n                                            NA_real_),\n                 match = if_else(match == \"matched\" & min_dist_unmatch &lt; dist & \n                                   !is.na(min_dist_unmatch), \"unmatched\", match),\n                 pseudonym = if_else(match == \"unmatched\", NA_character_, \n                                     pseudonym)) %&gt;%\n          ungroup()\n      }\n     \n      # Identify and adjust duplicate pseudonyms within the same year for matched cases\n      pool &lt;- pool %&gt;%\n        group_by(year, pseudonym) %&gt;%\n        mutate(dup_count = n()) %&gt;%\n        ungroup() %&gt;%\n        mutate(is_dup = ifelse(dup_count &gt; 1 & match == \"matched\", TRUE, FALSE)) %&gt;%\n        group_by(year, pseudonym) %&gt;%\n        mutate(dup_rank = ifelse(is_dup, row_number(), NA_integer_)) %&gt;%\n        ungroup() %&gt;%\n        mutate(match = ifelse(is_dup & dup_rank &gt; 1, \"unmatched\", match),\n               pseudonym = ifelse(is_dup & dup_rank &gt; 1, NA_character_, pseudonym)) %&gt;%\n        select(-dup_count, -is_dup, -dup_rank)\n\n      \n      pool$match &lt;- as.character(pool$match)\n      pool$pseudonym &lt;- as.character(pool$pseudonym)\n      \n      pseudonymized &lt;- pseudonymized %&gt;%\n        bind_rows(filter(pool, match == \"matched\"))\n      pool &lt;- filter(pool, match != \"matched\")\n      next_pseudonym_id &lt;- next_pseudonym_id + 1\n    }\n  }\n  return(pseudonymized)\n}\n\n# The following process is very long (~1h with a good computer)\n# We only run it once\npseudo_loc &lt;- \"output/pseudonymized_all.rds\"\n\nif (!file.exists(pseudo_loc)) {\n  # Set up parallel plan\n  plan(multisession, workers = 6)\n  \n  # Define processing function to include progress signaling\n  process_household_with_progress &lt;- function(household_id, .progress) {\n    .progress()  # Signal progress update\n    pool &lt;- all_data %&gt;% filter(j5 == household_id)\n    pseudonymized &lt;- pseudonymize_household(pool)\n    return(pseudonymized)\n  }\n  \n  # tic()\n  # Wrap processing in with_progress\n  pseudonymized_all &lt;- with_progress({\n    # Create a progressor function inside with_progress\n    p &lt;- progressor(along = household_ids)\n    \n    future_map_dfr(household_ids\n                   , ~process_household_with_progress(.x, p), .progress = FALSE)\n  })\n  # toc() # 3197.55 sec elapsed, 53 minutes\n  \n  write_rds(pseudonymized_all, pseudo_loc)\n} else { # Otherwise we read the existing milestone\n  pseudonymized_all &lt;- read_rds(pseudo_loc)\n}\n\nfor (year in years) {\n  \n  # Determine the file name based on the year\n  file_name &lt;- if (year == 1995) \"res_m.dta\" else \"res_m_a.dta\"\n  file_path &lt;- file.path(ros_data_loc, as.character(year), file_name)\n  \n  # Read the full dataset for the year\n  res_m &lt;- read_dta(file_path) %&gt;%\n    mutate(m5 = as.numeric(m5),  # Convert m5 to numeric\n           line_number = row_number())\n  \n  # Merge pseudonym information from pseudonymized_all\n  res_m_with_pseudonym &lt;- res_m %&gt;%\n    left_join(pseudonymized_all %&gt;% \n                select(m1, j5, m4, m5, year, line_number, pseudonym), \n              by = c(\"m1\", \"j5\", \"m4\", \"m5\", \"line_number\", \"year\")) %&gt;%\n    relocate(pseudonym, .after = m1) %&gt;% # Move pseudonym column after m1 if needed\n    select(-m1, -line_number)\n  \n  # Check for missing pseudonym values\n  missing_pseudonyms &lt;- sum(is.na(res_m_with_pseudonym$pseudonym))\n  if (missing_pseudonyms &gt; 0) {\n    stop(paste(\"Error: Missing pseudonym values found in year\", year, \n               \"- Total missing:\", missing_pseudonyms))\n  }\n  \n  # Write the dataset back to a Stata file\n  write_dta(res_m_with_pseudonym, file_path)\n}\n\n\nAfter this process, the column “m1” containing the name of household members has been removed from all the data files and it has been replaced by the column “pseudonym”. While anonymizing the data, this process enabled the tracking of repeated observations of the same individuals. We are now able to compute the number of unique individuals that have been surveyed throughout the years.\n\n\nCode\npseudo_loc &lt;- \"output/pseudonymized_all.rds\"\npseudonymized_all &lt;- read_rds(pseudo_loc)\n\n# Total number of individual observations\ntotal_individual_observations &lt;- nrow(pseudonymized_all)\n\n# Total number of unique households\nunique_households &lt;- pseudonymized_all %&gt;% \n  distinct(j5) %&gt;% \n  nrow()\n\n# Total number of household observations across years\nhousehold_observations_across_years &lt;- pseudonymized_all %&gt;% \n  group_by(year) %&gt;% \n  summarise(n_distinct_j5 = n_distinct(j5)) %&gt;% \n  summarise(total = sum(n_distinct_j5))\n\n# Average number of times a household was surveyed\naverage_surveys_per_household &lt;- pseudonymized_all %&gt;% \n  group_by(j5) %&gt;% \n  summarise(n_surveys = n_distinct(year)) %&gt;% \n  summarise(average = mean(n_surveys))\n\n# Number of unique individuals (considering both household ID and pseudonym)\nunique_individuals &lt;- pseudonymized_all %&gt;% \n  distinct(j5, pseudonym) %&gt;% \n  nrow()\n\n# Average number of times an individual was surveyed\naverage_surveys_per_individual &lt;- total_individual_observations / unique_individuals\n\n# Creating a summary table\nsummary_table &lt;- tibble(\n  Unique_households = unique_households,\n  Household_observations = household_observations_across_years$total,\n  Average_surveys_per_household = average_surveys_per_household$average,  \n  Unique_individuals = unique_individuals,\n  Individual_observations = total_individual_observations,\n  Average_surveys_per_individual = average_surveys_per_individual) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"Metric\", values_to = \"Value\") %&gt;%\n  mutate(Metric = str_replace_all(Metric, \"_\", \" \"),\n         FormattedValue = case_when(\n           Value == floor(Value) ~ format(as.integer(Value), big.mark = \",\"),\n           TRUE ~ sprintf(\"%.2f\", Value)))\n\ngt(summary_table)  %&gt;%\n  cols_label(FormattedValue = \"Value\") %&gt;% # Renaming 'Value' to 'Formatted Value' for display\n  cols_hide(column = \"Value\")\n\n\n\n\n\n\nTable 4.1:  Number of observations and unique entities in the ROR data from 1995\nto 2015 \n  \n    \n    \n      Metric\n      Value\n    \n  \n  \n    Unique households\n 30,972\n    Household observations\n102,733\n    Average surveys per household\n3.32\n    Unique individuals\n228,060\n    Individual observations\n598,965\n    Average surveys per individual\n2.63\n  \n  \n  \n\n\n\n\n\nAs we see in Table 4.1, we have 598,965 unique individuals who were surveyed an average of 2.63 times.\n\n\n4.2.2 Anonymization of surveyors\nThe Surveyors, supervisors and data entry clerks id numbers are included in the datasets. For the years 2011 to 2015, their names have also been included. We remove them before data publication.\n\n\nCode\nfor (year in  2011:2015) {\n  loc &lt;- paste0(\"data/dta_format/\", year, \"/res_deb.dta\")\n  df &lt;- read_dta(loc) %&gt;%\n    select(-j1_a, -j2_a, -j3_a) %&gt;%\n    write_dta(loc)\n}\n\n\nAt this stage, there is not any personal name in the survey dataset."
  },
  {
    "objectID": "04-ros-data-preparation.html#table-labels",
    "href": "04-ros-data-preparation.html#table-labels",
    "title": "4  Data preparation",
    "section": "4.3 Table labels",
    "text": "4.3 Table labels\nLabels are included for all variables in the raw data, but not for the tables. We manually recoded them from the questionnaires and includes them in the STATA files.\n\n\nCode\n# Load the Excel file containing file names and labels\nfile_labels &lt;- read_excel(\"references/file_labels.xlsx\")\n\n# Check and trim labels to 80 characters, and collect filenames needing trimming\nfile_labels &lt;- file_labels %&gt;% \n  mutate(needs_trimming = nchar(title_en) &gt; 80,\n         title_en = if_else(needs_trimming, substr(title_en, 1, 80), title_en))\nfiles_with_trimmed_labels &lt;- file_labels %&gt;% \n  filter(needs_trimming) %&gt;%\n  pull(filename)\n\n# Warn if any labels were trimmed\nif (length(files_with_trimmed_labels) &gt; 0) {\n  warning(\"Labels for the following files were trimmed to 80 characters: \", paste(files_with_trimmed_labels, collapse = \", \"))\n}\n# res_ccp3.dta, rx_jn.dta, rx_tj4.dta, rx_tj5.dta \n\n\n# Define the base path for  folders\nbase_path &lt;- \"data/dta_format\"\n\n# Get the list of yearly folders using base R\nyear_folders &lt;- list.dirs(base_path, full.names = TRUE, recursive = FALSE)\n\n# Function to read, check label length, and write .dta files\nprocess_files &lt;- function(year_folder) {\n  dta_files &lt;- list.files(year_folder, pattern = \"\\\\.dta$\", full.names = TRUE)\n  \n  purrr::walk(dta_files, function(file_path) {\n    file_name &lt;- basename(file_path)\n    \n    # Find corresponding label in the file_labels dataframe\n    label &lt;- file_labels %&gt;%\n      filter(filename == file_name) %&gt;%\n      pull(title_en) %&gt;%\n      first()\n    \n    # Proceed only if label is found\n    if (!is.na(label)) {\n      data &lt;- read_dta(file_path)\n      # Write the .dta file back with the new label\n      write_dta(data, file_path, label = label)\n    } \n  })\n}\n\n# Process files in each year folder\npurrr::walk(year_folders, process_files)\n\n\nThe table labels can now be included in the data catalog published with the survey."
  },
  {
    "objectID": "04-ros-data-preparation.html#data-format-conversions",
    "href": "04-ros-data-preparation.html#data-format-conversions",
    "title": "4  Data preparation",
    "section": "4.4 Data format conversions",
    "text": "4.4 Data format conversions\nThe ROS survey data was originally entered and managed in STATA, which is a proprietary format. To facilitate processing by all users, we also publish the data in an open format (tabulation-separated values, tsv).\n\n\nCode\n# Define source and target directories\nsource_dir &lt;- \"data/dta_format\"\ntarget_dir &lt;- \"data/tsv_format\"\n\n# Remove the target directory if it exists\nif (dir_exists(target_dir)) {\n  dir_delete(target_dir)\n}\n\n# Create the target directory\ndir_create(target_dir)\n\n# Get the list of year directories\nyear_dirs &lt;- dir_ls(source_dir)\n\nfor (year_dir in year_dirs) {\n  # Extract the year from the path\n  year &lt;- basename(year_dir)\n  \n  # Create the corresponding year directory in the target\n  target_year_dir &lt;- file.path(target_dir, year)\n  dir_create(target_year_dir)\n  \n  # Get a list of Stata files in the current year directory\n  stata_files &lt;- dir_ls(year_dir, glob = \"*.dta\")\n  \n  for (stata_file in stata_files) {\n    # Read the Stata file, preserving variable labels\n    data &lt;- read_dta(stata_file)\n    \n    # Convert labelled variables to factors where applicable\n    data &lt;- data %&gt;% \n      mutate(across(where(~is.labelled(.x)), as_factor))\n    \n    # Define the output TSV file path\n    tsv_file &lt;- file.path(target_year_dir, paste0(basename(stata_file), \".tsv\"))\n    \n    # Save the data frame as a TSV file\n    write_tsv(data, tsv_file)\n  }\n}\n\n\nThe data is now ready to be uploaded in both formats: the STATA proprietary format and the tsv open format.\n\n\n\n\nJaro, Matthew A. 1989. “Advances in Record-Linkage Methodology as Applied to Matching the 1985 Census of Tampa, Florida.” Journal of the American Statistical Association 84 (406): 414–20. https://doi.org/10.1080/01621459.1989.10478785.\n\n\nWinkler, William E. 1990. “String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage.” https://eric.ed.gov/?id=ED325505."
  },
  {
    "objectID": "05-bibliography.html",
    "href": "05-bibliography.html",
    "title": "References",
    "section": "",
    "text": "Gubert, Flore, and Anne-Sophie Robilliard. 2008. “Risk and\nSchooling Decisions in Rural Madagascar: A Panel Data-Analysis.”\nJournal of African Economies 17 (2): 207–38. https://doi.org/10.1093/jae/ejm010.\n\n\nJaro, Matthew A. 1989. “Advances in Record-Linkage Methodology as\nApplied to Matching the 1985 Census of Tampa, Florida.”\nJournal of the American Statistical Association 84 (406):\n414–20. https://doi.org/10.1080/01621459.1989.10478785.\n\n\nVaillant, Julia. 2013. “Attrition and Follow-Up Rules in Panel\nSurveys: Insights from a Tracking Experience in Madagascar.”\nReview of Income and Wealth 59 (3): 509–38. https://doi.org/10.1111/j.1475-4991.2012.00505.x.\n\n\nWinkler, William E. 1990. “String Comparator Metrics and Enhanced\nDecision Rules in the Fellegi-Sunter Model of Record Linkage.” https://eric.ed.gov/?id=ED325505."
  },
  {
    "objectID": "05-ros-included-modules.html",
    "href": "05-ros-included-modules.html",
    "title": "5  Load required libraries",
    "section": "",
    "text": "We create a simple plot to represent the years in which each survey modules were included in the survey questionnaire.\n\n\nCode\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n# Load the CSV file\ndata &lt;- read_csv(\"references/modules_en.csv\") %&gt;%\n  rename(Module = ...1)\n\n# Convert the data to long format\ndata_long &lt;- data %&gt;%\n  pivot_longer(cols = starts_with(\"19\") | starts_with(\"20\"), names_to = \"Year\", \n               values_to = \"Present\")\n\n# Convert 'Present' to logical for easier filtering and ordering\ndata_long &lt;- data_long %&gt;%\n  mutate(Present = as.logical(Present))\n\n# Create the lollipop chart\nmodule_plot &lt;- ggplot(data_long %&gt;% filter(Present), \n               aes(x = Year, y = factor(Module, levels = rev(data$Module)))) +\n  geom_segment(aes(xend = Year, yend = Module), size = 1) +\n  geom_point(size = 4, color = \"black\") +\n  theme_minimal() +\n  labs(title = NULL, x = NULL, y = NULL) +\n  theme(axis.text.y = element_text(size = 8), \n        legend.position = \"none\",\n        plot.title = element_blank(),\n        plot.subtitle = element_blank(),\n        axis.text.x = element_text(angle = 90, hjust = 1),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.x.top = element_text(angle = 90, vjust = 0.5, hjust = 0)) +\n  scale_x_discrete(position = \"top\")\n\nggsave(filename = \"figure_2.pdf\", \n       plot = module_plot,\n       path = \"output\",\n       width = 1800, height = 1800, units = \"px\", dpi = 300)\nggsave(filename = \"figure_2.png\", \n       plot = module_plot,\n       path = \"output\",\n       width = 1800, height = 1800, units = \"px\", dpi = 300)\n\nmodule_plot"
  },
  {
    "objectID": "06-ros-cost-updating.html#objective",
    "href": "06-ros-cost-updating.html#objective",
    "title": "6  ROS costs updating and comparison",
    "section": "6.1 Objective",
    "text": "6.1 Objective\nWe find the following report on the is the original annual budget of the rural observatories from 1995 to 1998 in Droy, Ratovoarinony and Roubaud (2000, 133):\n\n\n\nAnnual budget of the rural observatories, 1995-1998 in French francs\n\n\nThe objective of this analysis is to update and compare the annual costs of implementing rural observatories in Madagascar from 1995 to 1998 with current prices in Malagasy Ariary and Euros.\nThe process involves:\n\nConverting historical costs from French francs to Malagasy francs using historical exchange rates ;\nAdjusting these values for inflation using annual consumer price indices from the World Bank ;\nConverting the inflation-adjusted values to Euros using the latest exchange rates."
  },
  {
    "objectID": "06-ros-cost-updating.html#procedure",
    "href": "06-ros-cost-updating.html#procedure",
    "title": "6  ROS costs updating and comparison",
    "section": "6.2 Procedure",
    "text": "6.2 Procedure\n\n6.2.1 Step 1: enter the original table data\nWe re-create the budget data by copying the original figure.\n\n\nCode\n# Load tidyverse for easier data manipulation\nlibrary(tidyverse)\nlibrary(gt)\noptions(scipen = 999) # on désactive les notations scientifiques\n# Data from the image\nbudget_data1 &lt;- tibble(\n  Year = c(1995, 1996, 1997, 1998),\n  Preparation_of_field_operations = c(12250, NA, NA, NA),\n  Reproduction_of_questionnaires = c(5470, 5570, 4050, 3690),\n  Vehicle_running_costs = c(5010, 17130, 7010, 6620),\n  Payment_of_team_of_data_loggers = c(8330, 8120, 8430, 6680),\n  Payment_of_data_collection_team = c(52600, 57080, 58240, 69980),\n  Supplies_equipment_and_rent = c(6860, 5260, 3300, 3740),\n  Publication_of_initial_results = c(19710, 16810, 15270, 14260),\n  Total = c(110230, 109970, 96300, 104970)\n)\ngt(budget_data1)\n\n\n\n\n\n\n  \n    \n    \n      Year\n      Preparation_of_field_operations\n      Reproduction_of_questionnaires\n      Vehicle_running_costs\n      Payment_of_team_of_data_loggers\n      Payment_of_data_collection_team\n      Supplies_equipment_and_rent\n      Publication_of_initial_results\n      Total\n    \n  \n  \n    1995\n12250\n5470\n5010\n8330\n52600\n6860\n19710\n110230\n    1996\nNA\n5570\n17130\n8120\n57080\n5260\n16810\n109970\n    1997\nNA\n4050\n7010\n8430\n58240\n3300\n15270\n96300\n    1998\nNA\n3690\n6620\n6680\n69980\n3740\n14260\n104970"
  },
  {
    "objectID": "06-ros-cost-updating.html#step-2-convert-french-francs-to-malagasy-francs",
    "href": "06-ros-cost-updating.html#step-2-convert-french-francs-to-malagasy-francs",
    "title": "6  ROS costs updating and comparison",
    "section": "6.3 Step 2: Convert French Francs to Malagasy Francs",
    "text": "6.3 Step 2: Convert French Francs to Malagasy Francs\nNote that both currency have been discarded for Euros and Ariary respectively.\n\n\nCode\n# Exchange rates\nexchange_rates &lt;- tibble(\n  Year = c(1995, 1996, 1997, 1998),\n  Exchange_Rate = c(900, 797, 884, 936)\n)\n\n# Join exchange rates and convert French francs to Malagasy francs\nbudget_data2 &lt;- budget_data1 %&gt;%\n  left_join(exchange_rates, by = \"Year\") %&gt;%\n  mutate(across(starts_with(\"Preparation_of_field_operations\"):starts_with(\"Total\"), \n                ~ .x * Exchange_Rate))\ngt(budget_data2)\n\n\n\n\n\n\n  \n    \n    \n      Year\n      Preparation_of_field_operations\n      Reproduction_of_questionnaires\n      Vehicle_running_costs\n      Payment_of_team_of_data_loggers\n      Payment_of_data_collection_team\n      Supplies_equipment_and_rent\n      Publication_of_initial_results\n      Total\n      Exchange_Rate\n    \n  \n  \n    1995\n11025000\n4923000\n4509000\n7497000\n47340000\n6174000\n17739000\n99207000\n900\n    1996\nNA\n4439290\n13652610\n6471640\n45492760\n4192220\n13397570\n87646090\n797\n    1997\nNA\n3580200\n6196840\n7452120\n51484160\n2917200\n13498680\n85129200\n884\n    1998\nNA\n3453840\n6196320\n6252480\n65501280\n3500640\n13347360\n98251920\n936"
  },
  {
    "objectID": "06-ros-cost-updating.html#step-3-retrieve-inflation-data-from-world-bank",
    "href": "06-ros-cost-updating.html#step-3-retrieve-inflation-data-from-world-bank",
    "title": "6  ROS costs updating and comparison",
    "section": "6.4 Step 3: Retrieve inflation data from World Bank",
    "text": "6.4 Step 3: Retrieve inflation data from World Bank\nWe use the WDI package and download the indicator named “Inflation, consumer prices (annual %) - Madagascar”, originates from the International Monetary Fund according to the dataset metadata: https://data.worldbank.org/indicator/FP.CPI.TOTL.ZG?locations=MG\n\n\nCode\n# Load the World data indicator package to query world bank API\nlibrary(WDI)\n# Download inflation data from the World Bank\nif (!file.exists(\"references/inflation_data.rds\")) {\n  inflation_data &lt;- WDI(country = \"MG\", indicator = \"FP.CPI.TOTL.ZG\", start = 1995, end = 2023) %&gt;%\n  rename(Year = year, Inflation = FP.CPI.TOTL.ZG)\nwrite_rds(inflation_data, \"references/inflation_data.rds\")\n} else {\n  inflation_data &lt;- read_rds(\"references/inflation_data.rds\")\n}"
  },
  {
    "objectID": "06-ros-cost-updating.html#step-4-apply-inflation-rates-and-convert-to-ariary",
    "href": "06-ros-cost-updating.html#step-4-apply-inflation-rates-and-convert-to-ariary",
    "title": "6  ROS costs updating and comparison",
    "section": "6.5 Step 4: Apply inflation rates and convert to Ariary",
    "text": "6.5 Step 4: Apply inflation rates and convert to Ariary\nAs of date, the most recent year for annual inflation is 2023. Conversion factor from Malagasy franc to Ariary is 1 Ariary = 5 Malagasy francs.\n\n\nCode\n# Calculate cumulative inflation from the year to the most recent year \ncurrent_year &lt;- max(inflation_data$Year)\ncumulative_inflation &lt;- inflation_data %&gt;%\n  filter(Year &gt;= 1995) %&gt;%\n  arrange(desc(Year)) %&gt;%\n  mutate(Cumulative_Inflation = cumprod(1 + Inflation / 100)) %&gt;%\n  arrange(Year)\n\n# Apply cumulative inflation to convert Malagasy francs to current Ariary\nbudget_data3 &lt;- budget_data2 %&gt;%\n  left_join(cumulative_inflation, by = \"Year\") %&gt;%\n  mutate(across(starts_with(\"Preparation_of_field_operations\"):starts_with(\"Total\"), \n                ~ .x * Cumulative_Inflation / 5)) \n\ngt(budget_data3)\n\n\n\n\n\n\n  \n    \n    \n      Year\n      Preparation_of_field_operations\n      Reproduction_of_questionnaires\n      Vehicle_running_costs\n      Payment_of_team_of_data_loggers\n      Payment_of_data_collection_team\n      Supplies_equipment_and_rent\n      Publication_of_initial_results\n      Total\n      Exchange_Rate\n      country\n      iso2c\n      iso3c\n      Inflation, consumer prices (annual %)\n      Cumulative_Inflation\n    \n  \n  \n    1995\n34368751\n15346699\n14056118\n23370751\n147575208\n19246500\n55298619\n309262646\n900\nMadagascar\nMG\nMDG\n49.080210\n15.586735\n    1996\nNA\n9282793\n28548339\n13532546\n95127797\n8766156\n28015036\n183272667\n797\nMadagascar\nMG\nMDG\n19.756355\n10.455268\n    1997\nNA\n6251351\n10820239\n13012071\n89895969\n5093693\n23569908\n148643232\n884\nMadagascar\nMG\nMDG\n4.486383\n8.730449\n    1998\nNA\n5771771\n10354777\n10448626\n109460311\n5849979\n22305002\n164190467\n936\nMadagascar\nMG\nMDG\n6.208016\n8.355586"
  },
  {
    "objectID": "06-ros-cost-updating.html#step-5-convert-to-euros",
    "href": "06-ros-cost-updating.html#step-5-convert-to-euros",
    "title": "6  ROS costs updating and comparison",
    "section": "6.6 Step 5: Convert to euros",
    "text": "6.6 Step 5: Convert to euros\n\n\nCode\nif (!file.exists(\"references/exchange_rate_usd.rds\")) {\n  # Fetch the most recent exchange rate from MGA to USD\n  exchange_rate_usd &lt;- WDI(country = \"MG\", indicator = \"PA.NUS.FCRF\", start = 2023, end = 2023) %&gt;%\n    rename(Year = year, Exchange_Rate_to_USD = PA.NUS.FCRF)\n  write_rds(exchange_rate_usd, \"references/exchange_rate_usd.rds\")\n} else {\n  exchange_rate_usd &lt;- read_rds(\"references/exchange_rate_usd.rds\")\n}\nif (!file.exists(\"references/exchange_rate_euro.rds\")) {\n  # Fetch the most recent exchange rate from USD to Euro\n  exchange_rate_euro &lt;- WDI(country = \"FR\", indicator = \"PA.NUS.FCRF\", start = 2023, end = 2023) %&gt;%\n    rename(Year = year, Exchange_Rate_to_Euro = PA.NUS.FCRF)\n  write_rds(exchange_rate_euro, \"references/exchange_rate_euro.rds\")\n} else {\n  exchange_rate_euro &lt;- read_rds(\"references/exchange_rate_euro.rds\")\n}\n\n# Calculate the exchange rate from MGA to Euro\nariary_to_euro &lt;- exchange_rate_usd$Exchange_Rate_to_USD * \n  (1 / exchange_rate_euro$Exchange_Rate_to_Euro)\n\n# Convert to Euros\nbudget_data_euros &lt;- budget_data3 %&gt;%\n  mutate(across(starts_with(\"Preparation_of_field_operations\"):starts_with(\"Total\"), \n                ~ .x / ariary_to_euro))\ngt(budget_data_euros)\n\n\n\n\n\n\n  \n    \n    \n      Year\n      Preparation_of_field_operations\n      Reproduction_of_questionnaires\n      Vehicle_running_costs\n      Payment_of_team_of_data_loggers\n      Payment_of_data_collection_team\n      Supplies_equipment_and_rent\n      Publication_of_initial_results\n      Total\n      Exchange_Rate\n      country\n      iso2c\n      iso3c\n      Inflation, consumer prices (annual %)\n      Cumulative_Inflation\n    \n  \n  \n    1995\n7175.756\n3204.195\n2934.738\n4879.514\n30811.82\n4018.423\n11545.645\n64570.09\n900\nMadagascar\nMG\nMDG\n49.080210\n15.586735\n    1996\nNA\n1938.129\n5960.528\n2825.423\n19861.47\n1830.262\n5849.182\n38264.99\n797\nMadagascar\nMG\nMDG\n19.756355\n10.455268\n    1997\nNA\n1305.202\n2259.128\n2716.754\n18769.13\n1063.498\n4921.096\n31034.81\n884\nMadagascar\nMG\nMDG\n4.486383\n8.730449\n    1998\nNA\n1205.072\n2161.945\n2181.540\n22853.91\n1221.401\n4657.000\n34280.87\n936\nMadagascar\nMG\nMDG\n6.208016\n8.355586\n  \n  \n  \n\n\n\n\n\n6.6.1 Calculation summary in French for cross-validation\nBudget total de 1995 en Fmg : 99 207 000 francs malgaches (110 230 Francs français x taux de change de 900 Fmg/FF indiqués dans la publication). Budget total de 1995 en Ariary de 1995 : 19 841 400 Ariary (99 207 000 / 5)/ Budget total de 1995 en Ariary de 2023: 309 129 012 Ariary, le taux d’inflation composé de 1995 à 2023 est de 1 558 % (309 129 012 * 15.58). Budget total de 1995 en € de 2023: 69 781 €, le taux de change moyen en 2023 était de 4 430 Ar/€. Budget par observatoire en 1995 aux prix de 2023 : 17 445 Budget par ménage enquêté: 34,89 €"
  },
  {
    "objectID": "06-ros-cost-updating.html#complement-other-values-from-an-other-source",
    "href": "06-ros-cost-updating.html#complement-other-values-from-an-other-source",
    "title": "6  ROS costs updating and comparison",
    "section": "6.7 Complement: other values from an other source",
    "text": "6.7 Complement: other values from an other source\nDavid-Benz et al. report the following (2010, 36): “Le coût annuel du ROR s’élève actuellement à 210 000 eur (pour 15 observatoires)”. They do not specify which year this corresponds to, but the report was published in January 2010, there was no data collection in 2009 (political crisis in Madagascar) and the latest year in which there was 15 observatories was 2006, so we assume that it corresponds to 2006.\nWe convert this amount to 2006 Ariary, update with the cumulative inflation since 2007 and convert back to euros with 2023 currency rate.\n\n\nCode\n# Define the reported cost in 2006 euros\nreported_cost_eur_2006 &lt;- 210000\n\nif (!file.exists(\"references/exchange_rate_eur_to_usd_2006.rds\")) {\n  # Fetch the exchange rate from Euro to USD for 2006\n  exchange_rate_eur_to_usd_2006 &lt;- WDI(country = \"FR\", indicator = \"PA.NUS.FCRF\", \n                                       start = 2006, end = 2006) %&gt;%\n    pull(PA.NUS.FCRF)\n  write_rds(exchange_rate_usd_to_eur_2006, \n            \"references/exchange_rate_eur_to_usd_2006.rds\")\n  # Fetch the exchange rate from USD to MGA for 2006\n  exchange_rate_usd_to_mga_2006 &lt;- WDI(country = \"MG\", indicator = \"PA.NUS.FCRF\", \n                                       start = 2006, end = 2006) %&gt;%\n    pull(PA.NUS.FCRF)\n  write_rds(exchange_rate_usd_to_mga_2006, \n            \"references/exchange_rate_usd_to_mga_2006.rds\")\n} else {\n  exchange_rate_eur_to_usd_2006 &lt;- read_rds(\n    \"references/exchange_rate_eur_to_usd_2006.rds\")\n  exchange_rate_usd_to_mga_2006 &lt;- read_rds(\n    \"references/exchange_rate_usd_to_mga_2006.rds\")\n}\n\n# Convert the reported cost from euros to USD for 2006\nreported_cost_usd_2006 &lt;- reported_cost_eur_2006 * (1/exchange_rate_eur_to_usd_2006)\n\n# Convert the reported cost from USD to MGA for 2006\nreported_cost_mga_2006 &lt;- reported_cost_usd_2006 * exchange_rate_usd_to_mga_2006\n\n# Fetch the cumulative inflation from 2007 to 2023\ncumulative_inflation_2007_to_2023 &lt;- inflation_data %&gt;%\n  filter(Year &gt;= 2007) %&gt;%\n  arrange(Year) %&gt;%\n  mutate(Cumulative_Inflation = cumprod(1 + Inflation / 100)) %&gt;%\n  filter(Year == 2023) %&gt;%\n  pull(Cumulative_Inflation)\n\n# Update the 2006 MGA cost with cumulative inflation to 2023\nreported_cost_mga_2023 &lt;- reported_cost_mga_2006 * cumulative_inflation_2007_to_2023\n\n# Convert the updated MGA cost back to euros using the 2023 exchange rate\nreported_cost_eur_2023 &lt;- reported_cost_mga_2023 / ariary_to_euro\n\n# Print the updated cost in 2023 euros\nreported_cost_eur_2023\n\n\n[1] 408676.8\nattr(,\"label\")\n[1] \"Official exchange rate (LCU per US$, period average)\"\n\n\nCode\n# Print the updated 2023 cost per observatory in euros\ncost_per_obs_2023 &lt;- reported_cost_eur_2023 / 15\ncost_per_obs_2023 \n\n\n[1] 27245.12\nattr(,\"label\")\n[1] \"Official exchange rate (LCU per US$, period average)\"\n\n\nCode\n# Print the cost per household\ncost_per_obs_2023 / 500\n\n\n[1] 54.49024\nattr(,\"label\")\n[1] \"Official exchange rate (LCU per US$, period average)\"\n\n\n\n6.7.1 Calculation summary in French for cross-validation\n\nBudget 2006 en Ariary de 2006 : 564 900 000 Ariary (210 000€ * 2690 Ar/€ de taux de change moyen sur 2006)\nBudget 2006 en Ariary de 2023 : 1 960 203 000 Ariary (taux d’inflation cumulé de 347 % entre 2006 et 2023)\nBudget 2006 en euros de 2023 : 442 484 € (taux de change en 2023 de 4430 Ar/€4430)\nBudget 2006 par observatoire : 29499 € Budget 2006 par ménage enquêté : 59€\n\n\n\n\n\nDavid-Benz, Hélène, Michel Benoit-Cattin, and Rivo Ramboarison. 2010. Evaluation Du Réseau Des Observatoires Ruraux à Madagascar. Montpellier: CIRAD. https://agritrop.cirad.fr/558679/1/document_558679.pdf.\n\n\nDroy, Isabelle, Raphaël Ratovoarinony, and François Roubaud. 2000. “Les Observatoires Ruraux à Madagascar. Une Méthodologie Originale Pour Le Suivi Des Campagnes.” Stateco 95: 123140."
  },
  {
    "objectID": "07-bibliography.html",
    "href": "07-bibliography.html",
    "title": "References",
    "section": "",
    "text": "David-Benz, Hélène, Michel Benoit-Cattin, and Rivo Ramboarison. 2010.\nEvaluation Du Réseau Des Observatoires Ruraux à Madagascar.\nMontpellier: CIRAD. https://agritrop.cirad.fr/558679/1/document_558679.pdf.\n\n\nDroy, Isabelle, Raphaël Ratovoarinony, and François Roubaud. 2000.\n“Les Observatoires Ruraux à Madagascar. Une Méthodologie Originale\nPour Le Suivi Des Campagnes.” Stateco 95: 123140.\n\n\nGubert, Flore, and Anne-Sophie Robilliard. 2008. “Risk and\nSchooling Decisions in Rural Madagascar: A Panel Data-Analysis.”\nJournal of African Economies 17 (2): 207–38. https://doi.org/10.1093/jae/ejm010.\n\n\nJaro, Matthew A. 1989. “Advances in Record-Linkage Methodology as\nApplied to Matching the 1985 Census of Tampa, Florida.”\nJournal of the American Statistical Association 84 (406):\n414–20. https://doi.org/10.1080/01621459.1989.10478785.\n\n\nVaillant, Julia. 2013. “Attrition and Follow-Up Rules in Panel\nSurveys: Insights from a Tracking Experience in Madagascar.”\nReview of Income and Wealth 59 (3): 509–38. https://doi.org/10.1111/j.1475-4991.2012.00505.x.\n\n\nWinkler, William E. 1990. “String Comparator Metrics and Enhanced\nDecision Rules in the Fellegi-Sunter Model of Record Linkage.” https://eric.ed.gov/?id=ED325505."
  }
]