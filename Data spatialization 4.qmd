---
title: "Appendix: data spatialization"
author: "Florent Bédécarrats"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

## Cleanig and harmonizing the municipality

### Preparing the Data for Municipality Matching

To accurately merge datasets with varying administrative names, we often have to utilize a combination of automated processes and manual review. Our approach consists of the following major steps:

1.  **Reading and Preprocessing Survey Data:** Initial handling of raw survey data to extract and standardize municipality names.
2.  **Processing GADM Data:** GADM offers a comprehensive spatial dataset, which we leverage for authoritative municipality names.
3.  **Fuzzy Matching:** Given the potential discrepancies between names in different datasets, a fuzzy string matching technique is applied to assess the degree in which the municipality names found in the ROR survey differ from their closest match in GADM dataset.
4.  **Computing Relative Distance and Preparing for Manual Review:** The relative difference between matched strings is used to guide our manual review process.

```{r}
# Load required libraries
library(tidyverse)
library(haven)
library(labelled)
library(geodata)
library(sf)
library(stringdist)
library(tmap)
library(rnaturalearth)
library(cowplot)
library(viridis)
```

## Extract a list of location values

```{r}
# Define path to the survey location data
deb_files <- list.files(path = "enter", pattern = "res_deb", 
                        recursive = TRUE, 
                        full.names = TRUE) %>%
  str_subset("stunicode", negate = TRUE) 

# Define variables related to municipalty locations
location_variables <- list(
  j0 = "observatory code",
  obs = "observatory",
  j41 = "municipality code",
  j42 = "municipality name",
  j4_code = "village code",
  j4 = "village name",
  code_site = "site code")

# Extract location-related data from the survey files and preprocess
location_values <- deb_files %>%
  map(~ read_dta(.x) %>%
        mutate(across(where(is.labelled), ~ as.character(as_factor(.))))) %>%
  map(~ select(.x, any_of(names(location_variables)))) %>%
  map(~ .x %>% mutate_all(as.character)) %>%
  map(unique) %>%
  bind_rows() %>%
  rename(obs_code = j0, muni_code = j41, muni_ror = j42, hamlet_code = j4_code,
         hamlet = j4, site_code = code_site) %>%
  mutate(muni = str_to_upper(muni_ror),
         muni = str_replace_all(muni, "/", " "),
         muni = str_remove_all(muni, " CENTRE")) %>%
  unique()
```

As a resukts we have a list of all variation of location variables found in the data. We have a total of `r nrow(location_values)` different commune names.

### Processing GADM Data

The GADM (Global Administrative Areas) database provides up-to-date administrative boundaries and names for all countries globally. Here, we extract the municipal names for Madagascar. These serve as our reference set in the subsequent matching process.

```{r}
munis_gadm <- gadm("MDG", level  = 4, path = "data") %>%
  st_as_sf() %>%
  mutate(muni = str_to_upper(NAME_4))
```

### Fuzzy Matching Process

Given that different datasets might have slight variations in naming conventions or typographical differences, a simple direct match can often overlook correct pairings. By employing a fuzzy string matching method, we can capture these near-matches more effectively. Here, we compute a distance matrix to identify the closest possible matches between our two sets of municipality names.

```{r fuzzy_matching}
# Compute the distance matrix between municipality names in the survey data and GADM data
distance_matrix <- stringdistmatrix(location_values$muni, munis_gadm$muni, 
                                    method = "lv")

# Identify closest matches based on the distance matrix
closest_matches <- apply(distance_matrix, 1, which.min) %>%
  as.numeric()
proximity_grades_abs <- apply(distance_matrix, 1, min)
proximity_grades_rel <- proximity_grades_abs / str_length(location_values$muni) * 100

# Bind the municipality names, their closest matches, and the proximity grades into a tibble
muni_matches <- tibble(
  obs_code = location_values$obs_code,
  muni_ror = location_values$muni_ror,
  muni = location_values$muni,
  hamlet_code = location_values$hamlet_code,
  hamlet = location_values$hamlet,
  closest_gadm_name = munis_gadm$muni[closest_matches],
  closest_gadm_gid = munis_gadm$GID_4[closest_matches],
  closest_gadm_district = munis_gadm$NAME_3[closest_matches],
  closest_gadm_region = munis_gadm$NAME_2[closest_matches],
  proximity_grade_abs = proximity_grades_abs,
  proximity_grade_rel = proximity_grades_rel) %>%
  arrange(desc(proximity_grade_rel)) # Order by relative distance

# Export the dataset for manual review
writexl::write_xlsx(muni_matches, "review_muni_matches.xlsx")

```

We computed the Levenshtein distance between each municipality name from the survey data and the GADM data. Based on this distance, we identify the closest match from the GADM dataset for each survey municipality name. The proximity grade is calculated to guide potential manual review.

We check observatory by observatory to identify similar names of naming patterns. To disambiguate some location names, we also use the Populated places database from the OCHA (https://data.humdata.org/fr/dataset/madagascar-settlements)

### Re-incorporate the corrected matches

```{r}
# Reading the corrected file and keeping only the edits
reviewed_matches <- readxl::read_xlsx("review_muni_matches_correct.xlsx") 
# Matches without muni in the first place
no_muni <- reviewed_matches %>%
  filter(is.na(muni))
# Matches that didn't need a correction
correct_matches <- reviewed_matches %>%
  filter(!is.na(muni)) %>%
  filter(is.na(NAME_4_correct) & is.na(GID_4_correct))
# Corrected matches that already have a GID
corrected_matches_gid <- reviewed_matches %>%
  filter(!is.na(muni) & !is.na(GID_4_correct))
# Corrected matches where GID is missing
corrected_matches_nogid <- reviewed_matches %>%
  filter(!is.na(muni)) %>%
  filter(is.na(GID_4_correct) & !is.na(NAME_4_correct)) 
# Check that the number of lines of subsets equal the initial number of lines
if (nrow(reviewed_matches) != nrow(no_muni) + nrow(correct_matches) +
    nrow(corrected_matches_gid) + nrow(corrected_matches_nogid)) {
  print("Error, sizes of review subsets do not match review size")
}
# Add the GID for those
corrected_matches_addgid <- corrected_matches_nogid %>%
  select(-GID_4_correct) %>%
  left_join(munis_gadm %>%
              st_drop_geometry() %>%
              select(NAME_4_correct = muni, GID_4_correct = GID_4),
            by = join_by("NAME_4_correct"), multiple = "first")
# Consolidate all corrected matches
corrected_matches_all <- corrected_matches_gid %>%
  bind_rows(corrected_matches_addgid) %>%
  mutate(closest_gadm_name = NAME_4_correct,
         closest_gadm_gid = GID_4_correct) %>%
  bind_rows(correct_matches) %>%
  select(-closest_gadm_district, -closest_gadm_region, -proximity_grade_abs,
         -proximity_grade_rel, -NAME_4_correct, -GID_4_correct, -Source_correct)

# Keep only the municipalities for plotting
unique_muni_data <- corrected_matches_all %>%
  filter(!is.na(closest_gadm_gid)) %>%
  select(obs_code, closest_gadm_gid) %>%
  distinct()

unique_muni_data <- unique_muni_data %>%
  left_join(select(munis_gadm, 
                   GID_4, commune = NAME_4, district = NAME_3, region = NAME_2), 
  by = c("closest_gadm_gid" = "GID_4")) %>%
  st_as_sf()
  
tmap_mode("view")
tm_shape(unique_muni_data) +
  tm_borders() +
  tm_fill(col = "obs_code", palette = "Set3", title = "Observatory Code",
          popup.vars = c("closest_gadm_gid", "commune", "district", "region")) +
  tm_layout(main.title = "Municipalities with Data by Observatory Code")


tm_shape(munis_gadm %>%
           filter(NAME_3 == "Ambositra" | NAME_3 == "Ambatondrazaka")) +
  tm_fill(col = "yellow") +
  tm_shape(filter(unique_muni_data, obs_code == "19")) +
  tm_polygons(col = "red", title = "Observatory Code",
              popup.vars = c("closest_gadm_gid", "commune", "district", "region")) +
  tm_layout(main.title = "Municipalities with Data by Observatory Code")

```

## Retreive observatory name

```{r}




```

```{r}
location_values_all <- deb_files %>%
  map(~ read_dta(.x) %>%
        mutate(across(where(is.labelled), ~ as.character(as_factor(.))))) %>%
  map(~ select(.x, any_of(names(location_variables)))) %>%
  map(~ .x %>% mutate_all(as.character)) %>%
  bind_rows() %>%
  rename(obs_code = j0, muni_code = j41, muni_ror = j42, hamlet_code = j4_code,
         hamlet = j4, site_code = code_site) %>%
  mutate(muni = str_to_upper(muni_ror),
         muni = str_replace_all(muni, "/", " "),
         muni = str_remove_all(muni, " CENTRE"))

location_values_all %>%
  mutate(has_muni = !is.na(muni) & muni != "") %>%
  group_by(has_muni) %>%
  summarise(count = n())
# # A tibble: 2 × 2
#   has_muni count
#   <lgl>    <int>
# 1 FALSE    55777
# 2 TRUE     46968


# Checking for duplicated rows in location_values_clean
duplicated_rows_location_values_clean <- location_values_clean %>%
  filter(duplicated(select(., obs_code, muni_ror, hamlet_code, hamlet)) | 
           duplicated(select(., obs_code, muni_ror, hamlet_code, hamlet), fromLast = TRUE))

# Checking for duplicated rows in muni_matches_updated
duplicated_rows_muni_matches_updated <- muni_matches_updated %>%
  filter(duplicated(select(., obs_code, muni_ror, muni_code, hamlet_code, hamlet, site_code)) |
           duplicated(select(., obs_code, muni_ror, muni_code, hamlet_code, hamlet, site_code), fromLast = TRUE))

```

## Counting data per observatory

```{r}
# Define a function to load and count surveys per observatory for a given year
load_and_count <- function(year, factorize = FALSE) {
  # Define file path
  file_path <- paste0("Données ROR/enter/", year, "/res_deb.dta")
  
  # Load data
  data <- read_dta(file_path)
  
  # Extract label and convert to factors if option
  if (factorize) {
    data <- data %>%
      mutate(across(everything(), as.character),
             across(where(is.labelled), ~ as.character(as_factor(.))))
  }
  
  # Count surveys per observatory
  count_data <- data %>%
    group_by(j0) %>%
    summarise(survey_count = n()) %>%
    ungroup() %>%
    mutate(year = year)  # Add year column
  
  return(count_data)
}

# Generate a list of years
years <- 1995:2014

# Use purrr::map_df to loop through each year and bind results
obs_count <- map_df(years, load_and_count) %>%
  # Remove rows with observatory "7 " and "NA", whch are errors
  filter(j0 != 7 & !is.na(j0)) %>%
  rename(observatory = j0)

# Read observatory names
observatory_names <- readxl::read_xlsx("Observatory_names.xlsx") %>%
  select(code, observatory_name = name)

# PAss it to wide.
obs_count <- obs_count %>%
  left_join(observatory_names, by = c("observatory" = "code")) %>%
  group_by(observatory_name, year) %>%
  summarise(survey_count = sum(survey_count))

obs_count_wide <- obs_count %>%
  pivot_wider(names_from = year, values_from = survey_count)


# Add observatory approximate location
locations <- tibble(
  code = c(1, 2, 3, 4, 12, 13, 15, 16, 21, 22, 23, 24, 31, 25, 41, 42, 43, 51, 
           44, 45, 61, 17, 18, 19, 71, 52),
  name = c("Antalaha", "Antsirabe", "Marovoay", "Toliara", "Antsohihy", 
           "Tsiroanomandidy-Bongo", "Farafangana", "Ambovombe", 
           "Alaotra", "Manjakandriana", "Toliara littoral", 
           "Fenerive Est", "Bekily", "Mahanoro", "Soaviandriana-Itasy", 
           "Menabe-Belo", "Fianarantsoa", "Tsivory", "Morondava", "Manandriana", 
           "Tanandava", "Ihosy", "Ambohimahasoa", "Manakara", "Tolanaro", 
           "Menabe-Nord-Est"),
  latitude = c(-14.8833, -19.8659, -16.1000, -23.3558, -14.8796, -18.7713, 
               -22.8167, -25.1667, -17.8319, -18.9167, -23.3558, -17.3500, 
               -24.6900, -19.9000, -19.1686, -19.6975, -21.4527, -24.4667, 
               -20.2833, -20.2333, -22.5711, -22.4000, -20.7145, -22.1333, 
               -25.0381, -20.5486),
  longitude = c(50.2833, 47.0333, 46.6333, 43.6683, 47.9875, 46.0546, 47.8333, 
                46.0833, 48.4167, 47.8000, 43.6683, 49.4167, 45.1700, 48.8000,
                46.7354, 44.5419, 47.0857, 45.4667, 44.2833, 47.3833, 45.0439, 
                46.1167, 47.0389, 48.0167, 46.9562, 47.1597))

obs_count <- left_join(obs_count, locations, by = c("observatory_name" = "name"))


madagascar <- st_read("../Entités administratives/OCHA_BNGRC frontières administratives/mdg_admbnda_adm0_BNGRC_OCHA_20181031.shp")

# Sort locations by latitude to generate sequence numbers
locations <- locations %>%
  arrange(desc(latitude)) %>%
  mutate(seq_num = 1:n())


# Create map plot with labels
map_plot <- ggplot(data = madagascar) +
  geom_sf(fill = "lightgray") +
  geom_point(data = locations, aes(x = longitude, y = latitude, color = name), size = 3) +
  geom_text(data = locations, aes(x = longitude, y = latitude, label = seq_num), 
            vjust = -1, hjust = 1, size = 3) + 
  theme_void() +
  theme(legend.position = "none")

# Add sequence numbers to observatory names in obs_count dataframe
obs_count <- obs_count %>%
  left_join(locations %>%
              select(name, seq_num), 
            by = c("observatory_name" = "name")) %>%
  mutate(observatory_with_num = paste0(seq_num, ". ", observatory_name))

# Create timeline plot using modified obs_count with observatory_with_num
timeline_plot <- ggplot(obs_count, aes(x = year, y = fct_reorder(observatory_with_num, latitude), color = observatory_name)) +
  geom_point(aes(size = survey_count), show.legend = F) +
  theme_minimal() +
  labs(y = NULL, x = NULL) +
  theme(axis.text.y = element_text(size = 8),
        legend.position = "none")

# Stitch the plots together
combined_plot <- plot_grid(map_plot, timeline_plot, rel_widths = c(1.3, 2))

print(combined_plot)


ggsave("../../Protocole/Figures/ROR_history.png", plot = combined_plot, 
       width = 10, height = 7, dpi = 300)

```

```{r}
# Sort locations by latitude to generate sequence numbers
locations <- locations %>%
  arrange(desc(latitude)) %>%
  mutate(seq_num = 1:n())

# Create map plot with labels
map_plot <- ggplot(data = madagascar) +
  geom_sf(fill = "lightgray") +
  geom_point(data = locations, aes(x = longitude, y = latitude, color = name), 
             size = 4) +
  geom_text(data = locations, aes(x = longitude, y = latitude, label = seq_num), 
            vjust = -1, hjust = 1, size = 3) + 
  theme_void() +
  theme(legend.position = "none")

# Add sequence numbers to observatory names in obs_count dataframe
obs_count <- obs_count %>%
  left_join(locations %>%
              select(observatory_name = name, seq_num), 
            by = "observatory_name") %>%
  mutate(observatory_with_num = paste0(seq_num, ". ", observatory_name))

# Create timeline plot using modified obs_count with observatory_with_num
timeline_plot <- ggplot(obs_count, aes(x = year, y = fct_reorder(observatory_with_num, latitude), color = observatory_name)) +
  geom_point(aes(size = 3), show.legend = F) +
  theme_minimal() +
  labs(y = NULL, x = NULL) +
  theme(axis.text.y = element_text(size = 8),
        legend.position = "none")

# Stitch the plots together
combined_plot <- plot_grid(map_plot, timeline_plot, rel_widths = c(1.3, 2))

print(combined_plot)

```

## other

```{r}
# Generate a complete grid of all combinations of year and observatory
complete_grid <- expand.grid(year = unique(obs_count$year), 
                             observatory_name = unique(obs_count$observatory_name))

# Left join with your actual data to mark presence/absence of surveys
plot_data <- left_join(complete_grid, obs_count, by = c("year", "observatory_name"))

# Create a binary variable to indicate presence or absence of a survey
plot_data$has_survey <- ifelse(!is.na(plot_data$survey_count), 1, 0)

# Plot
ggplot(plot_data, aes(x = year, y = observatory_name, fill = as.factor(has_survey))) +
  geom_tile(color = "white") +
  scale_fill_manual(values = c("0" = "white", "1" = "blue"), 
                    name = "Survey Conducted",
                    breaks = c("1", "0"),
                    labels = c("Yes", "No")) +
  labs(
    title = "Presence/Absence of Surveys by Observatories Over the Years",
    subtitle = "1995 to 2015",
    x = "Year",
    y = "Observatory"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.title = element_text(size = 12, face = "bold"),
    legend.text = element_text(size = 10),
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 14, hjust = 0.5),
    axis.title.x = element_text(size = 14, face = "bold"),
    axis.title.y = element_text(size = 14, face = "bold"),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12)
  )

```

## Create a data dictionnary

```{r}
# Function to extract variable info for a given year and file
extract_variable_info <- function(year, file) {
  # Define file path
  file_path <- paste0("enter/", year, "/", file)
  
  # Check if the file exists
  if (!file.exists(file_path)) return(NULL)
  
  # Load data (but only headers, not full data to speed up processing)
  data <- read_dta(file_path, n_max = 0)
  
  # Extract variable names and labels
  var_names <- names(data)
  var_labels <- var_label(data)
  
  # Create a tibble to store extracted data
  tibble(
    file_name = file,
    variable_name = var_names,
    variable_label = var_labels,
    year = year
  )
}

# Use purrr::map_df to loop through each year, list its files, and extract info
all_vars <- map_df(years, function(y) {
  # List files for the specific year
  files_for_year <- list.files(paste0("Données ROR/enter/", y), pattern = "\\.dta$", full.names = FALSE)
  # Apply extract_variable_info function for each file of the year
  map_df(files_for_year, extract_variable_info, year = y)
})

# Convert any NULL values in variable_label to "NA"
all_vars$variable_label <- as.character(all_vars$variable_label)
all_vars$variable_label[is.na(all_vars$variable_label)] <- "NA"

# Consolidate the information
variable_dictionary <- all_vars %>%
  group_by(file_name, variable_name) %>%
  arrange(year) %>%  # Arranging by year can ensure that we capture the most frequent or earliest label
  summarise(
    variable_label = first(variable_label[variable_label != "NA"] %||% "NA"),
    years_present = list(unique(year))
  ) %>%
  ungroup()


# Print the variable dictionary
print(variable_dictionary)
# Convert the list column to a character column
variable_dictionary$years_present <- sapply(variable_dictionary$years_present, paste, collapse = ",")

# Now split and save the chunks to CSV as before:
num_rows <- nrow(variable_dictionary)
chunk_size <- 100

split_data <- split(variable_dictionary, ceiling(seq_len(num_rows)/chunk_size))

for(i in seq_along(split_data)) {
  utils::write.table(split_data[[i]], file = paste0("chunk_", i, ".csv"), row.names = FALSE, sep = ",", quote = TRUE)
}

```
